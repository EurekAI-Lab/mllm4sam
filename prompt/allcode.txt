<source code for your reference for transformers package qwen related>
# coding=utf-8
# Copyright 2024 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""PyTorch Qwen2-VL model."""

import math
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint
from torch.nn import CrossEntropyLoss, LayerNorm

from ...activations import ACT2FN
from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache
from ...generation import GenerationMixin
from ...modeling_attn_mask_utils import AttentionMaskConverter
from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput
from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS
from ...modeling_utils import PreTrainedModel
from ...utils import (
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    is_flash_attn_2_available,
    is_flash_attn_greater_or_equal_2_10,
    logging,
    replace_return_docstrings,
)
from .configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLVisionConfig


if is_flash_attn_2_available():
    from flash_attn import flash_attn_varlen_func

    from ...modeling_flash_attention_utils import _flash_attention_forward
else:
    flash_attn_varlen_func = None


logger = logging.get_logger(__name__)

_CONFIG_FOR_DOC = "Qwen2VLConfig"


@dataclass
class Qwen2VLCausalLMOutputWithPast(ModelOutput):
    """
    Base class for Qwen2VL causal language model (or autoregressive) outputs.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
            Language modeling loss (for next-token prediction).
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):
            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
            `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
            The rope index difference between sequence length and multimodal rope.
    """

    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    past_key_values: Optional[List[torch.FloatTensor]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    rope_deltas: Optional[torch.LongTensor] = None


class Qwen2VLRotaryEmbedding(nn.Module):
    def __init__(self, config: Qwen2VLConfig, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    def _dynamic_frequency_update(self, position_ids, device):
        """
        dynamic RoPE layers should recompute `inv_freq` in the following situations:
        1 - growing beyond the cached sequence length (allow scaling)
        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)
        """
        seq_len = torch.max(position_ids) + 1
        if seq_len > self.max_seq_len_cached:  # growth
            inv_freq, self.attention_scaling = self.rope_init_fn(
                self.config, device, seq_len=seq_len, **self.rope_kwargs
            )
            self.register_buffer("inv_freq", inv_freq, persistent=False)  # TODO joao: may break with compilation
            self.max_seq_len_cached = seq_len

        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset
            self.register_buffer("inv_freq", self.original_inv_freq, persistent=False)
            self.max_seq_len_cached = self.original_max_seq_len

    @torch.no_grad()
    def forward(self, x, position_ids):
        if "dynamic" in self.rope_type:
            self._dynamic_frequency_update(position_ids, device=x.device)

        # Core RoPE block. In contrast to other models, Qwen2_VL has different position ids for the grids
        # So we expand the inv_freq to shape (3, ...)
        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)
        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)
        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)
        device_type = x.device.type
        device_type = device_type if isinstance(device_type, str) and device_type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos()
            sin = emb.sin()

        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention
        cos = cos * self.attention_scaling
        sin = sin * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


# Copied from transformers.models.llama.modeling_llama.rotate_half
def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim=1):
    """Applies Rotary Position Embedding with Multimodal Sections to the query and key tensors (https://qwenlm.github.io/blog/qwen2-vl/).

    Explanation:
        Multimodal 3D rotary position embedding is an extension to 1D rotary position embedding. The input embedding
        sequence contains vision (images / videos) embedding and text embedding or just contains text embedding. For
        vision embedding part, we apply rotary position embedding on temporal, height and width dimension separately.
        Here we split the channel dimension to 3 chunks for the temporal, height and width rotary position embedding.
        For text embedding part, we just apply 1D rotary position embedding. The three rotary position index (temporal,
        height and width) of text embedding is always the same, so the text embedding rotary position embedding has no
        difference with modern LLMs.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`):
            The position indices of the tokens corresponding to the query and key tensors. For example, this can be
            used to pass offsetted position ids when working with a KV-cache.
        mrope_section(`List(int)`):
            Multimodal rope section is for channel dimension of temporal, height and width in rope calculation.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    mrope_section = mrope_section * 2
    cos = torch.cat([m[i % 3] for i, m in enumerate(cos.split(mrope_section, dim=-1))], dim=-1).unsqueeze(
        unsqueeze_dim
    )
    sin = torch.cat([m[i % 3] for i, m in enumerate(sin.split(mrope_section, dim=-1))], dim=-1).unsqueeze(
        unsqueeze_dim
    )

    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def apply_rotary_pos_emb_vision(
    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor
) -> Tuple[torch.Tensor, torch.Tensor]:
    orig_q_dtype = q.dtype
    orig_k_dtype = k.dtype
    q, k = q.float(), k.float()
    cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    q_embed = q_embed.to(orig_q_dtype)
    k_embed = k_embed.to(orig_k_dtype)
    return q_embed, k_embed


class VisionRotaryEmbedding(nn.Module):
    def __init__(self, dim: int, theta: float = 10000.0) -> None:
        super().__init__()
        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)

    def forward(self, seqlen: int) -> torch.Tensor:
        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
        freqs = torch.outer(seq, self.inv_freq)
        return freqs


class PatchEmbed(nn.Module):
    def __init__(
        self,
        patch_size: int = 14,
        temporal_patch_size: int = 2,
        in_channels: int = 3,
        embed_dim: int = 1152,
    ) -> None:
        super().__init__()
        self.patch_size = patch_size
        self.temporal_patch_size = temporal_patch_size
        self.in_channels = in_channels
        self.embed_dim = embed_dim

        kernel_size = [temporal_patch_size, patch_size, patch_size]
        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=False)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        target_dtype = self.proj.weight.dtype
        hidden_states = hidden_states.view(
            -1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size
        )
        hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)
        return hidden_states


class PatchMerger(nn.Module):
    def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = 2) -> None:
        super().__init__()
        self.hidden_size = context_dim * (spatial_merge_size**2)
        self.ln_q = LayerNorm(context_dim, eps=1e-6)
        self.mlp = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.GELU(),
            nn.Linear(self.hidden_size, dim),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.mlp(self.ln_q(x).view(-1, self.hidden_size))
        return x


class VisionMlp(nn.Module):
    def __init__(self, dim: int, hidden_dim: int, hidden_act: str) -> None:
        super().__init__()
        self.fc1 = nn.Linear(dim, hidden_dim)
        self.act = ACT2FN[hidden_act]
        self.fc2 = nn.Linear(hidden_dim, dim)

    def forward(self, x) -> torch.Tensor:
        return self.fc2(self.act(self.fc1(x)))


class VisionAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int = 16) -> None:
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.qkv = nn.Linear(dim, dim * 3, bias=True)
        self.proj = nn.Linear(dim, dim)

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    ) -> torch.Tensor:
        seq_length = hidden_states.shape[0]
        q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)
        if position_embeddings is None:
            logger.warning_once(
                "The attention layers in this model are transitioning from computing the RoPE embeddings internally "
                "through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed "
                "`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be "
                "removed and `position_embeddings` will be mandatory."
            )
            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)
            cos = emb.cos()
            sin = emb.sin()
        else:
            cos, sin = position_embeddings
        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)

        attention_mask = torch.full(
            [1, seq_length, seq_length], torch.finfo(q.dtype).min, device=q.device, dtype=q.dtype
        )
        for i in range(1, len(cu_seqlens)):
            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = 0

        q = q.transpose(0, 1)
        k = k.transpose(0, 1)
        v = v.transpose(0, 1)
        attn_weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.head_dim)
        attn_weights = attn_weights + attention_mask
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)
        attn_output = torch.matmul(attn_weights, v)
        attn_output = attn_output.transpose(0, 1)
        attn_output = attn_output.reshape(seq_length, -1)
        attn_output = self.proj(attn_output)
        return attn_output


class VisionFlashAttention2(nn.Module):
    def __init__(self, dim: int, num_heads: int = 16) -> None:
        super().__init__()
        self.num_heads = num_heads
        self.qkv = nn.Linear(dim, dim * 3, bias=True)
        self.proj = nn.Linear(dim, dim)

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    ) -> torch.Tensor:
        seq_length = hidden_states.shape[0]
        q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)
        if position_embeddings is None:
            logger.warning_once(
                "The attention layers in this model are transitioning from computing the RoPE embeddings internally "
                "through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed "
                "`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be "
                "removed and `position_embeddings` will be mandatory."
            )
            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)
            cos = emb.cos()
            sin = emb.sin()
        else:
            cos, sin = position_embeddings
        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)

        max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
        attn_output = flash_attn_varlen_func(q, k, v, cu_seqlens, cu_seqlens, max_seqlen, max_seqlen).reshape(
            seq_length, -1
        )
        attn_output = self.proj(attn_output)
        return attn_output


class VisionSdpaAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int = 16) -> None:
        super().__init__()
        self.num_heads = num_heads
        self.qkv = nn.Linear(dim, dim * 3, bias=True)
        self.proj = nn.Linear(dim, dim)

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    ) -> torch.Tensor:
        seq_length = hidden_states.shape[0]
        q, k, v = self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)
        if position_embeddings is None:
            logger.warning_once(
                "The attention layers in this model are transitioning from computing the RoPE embeddings internally "
                "through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed "
                "`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be "
                "removed and `position_embeddings` will be mandatory."
            )
            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)
            cos = emb.cos()
            sin = emb.sin()
        else:
            cos, sin = position_embeddings
        q, k = apply_rotary_pos_emb_vision(q, k, cos, sin)

        attention_mask = torch.zeros([1, seq_length, seq_length], device=q.device, dtype=torch.bool)
        for i in range(1, len(cu_seqlens)):
            attention_mask[..., cu_seqlens[i - 1] : cu_seqlens[i], cu_seqlens[i - 1] : cu_seqlens[i]] = True
        q = q.transpose(0, 1)
        k = k.transpose(0, 1)
        v = v.transpose(0, 1)
        attn_output = F.scaled_dot_product_attention(q, k, v, attention_mask, dropout_p=0.0)
        attn_output = attn_output.transpose(0, 1)
        attn_output = attn_output.reshape(seq_length, -1)
        attn_output = self.proj(attn_output)
        return attn_output


QWEN2_VL_VISION_ATTENTION_CLASSES = {
    "eager": VisionAttention,
    "flash_attention_2": VisionFlashAttention2,
    "sdpa": VisionSdpaAttention,
}


class Qwen2VLVisionBlock(nn.Module):
    def __init__(self, config, attn_implementation: str = "sdpa") -> None:
        super().__init__()
        self.norm1 = LayerNorm(config.embed_dim, eps=1e-6)
        self.norm2 = LayerNorm(config.embed_dim, eps=1e-6)
        mlp_hidden_dim = int(config.embed_dim * config.mlp_ratio)

        self.attn = QWEN2_VL_VISION_ATTENTION_CLASSES[attn_implementation](
            config.embed_dim, num_heads=config.num_heads
        )
        self.mlp = VisionMlp(dim=config.embed_dim, hidden_dim=mlp_hidden_dim, hidden_act=config.hidden_act)

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    ) -> torch.Tensor:
        hidden_states = hidden_states + self.attn(
            self.norm1(hidden_states),
            cu_seqlens=cu_seqlens,
            rotary_pos_emb=rotary_pos_emb,
            position_embeddings=position_embeddings,
        )
        hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))
        return hidden_states


# Copied from transformers.models.qwen2.modeling_qwen2.Qwen2RMSNorm
class Qwen2RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        Qwen2RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


# Copied from transformers.models.qwen2.modeling_qwen2.Qwen2MLP
class Qwen2MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


# Copied from transformers.models.llama.modeling_llama.repeat_kv
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


class Qwen2VLAttention(nn.Module):
    """
    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer
    and "Generating Long Sequences with Sparse Transformers".
    """

    def __init__(self, config: Qwen2VLConfig, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"Instantiating {self.__class__.__name__} without passing `layer_idx` is not recommended and will "
                "to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )

        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.is_causal = True
        self.attention_dropout = config.attention_dropout
        self.rope_scaling = config.rope_scaling

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )
        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)

        self.rotary_emb = Qwen2VLRotaryEmbedding(config=config)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_multimodal_rotary_pos_emb(
            query_states, key_states, cos, sin, self.rope_scaling["mrope_section"]
        )

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

        if attention_mask is not None:  # no matter the length, we just slice it
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
            attn_weights = attn_weights + causal_mask

        # Fix precision issues in Qwen2-VL float16 inference
        # Replace inf values with zeros in attention weights to prevent NaN propagation
        if query_states.dtype == torch.float16:
            attn_weights = torch.where(torch.isinf(attn_weights), torch.zeros_like(attn_weights), attn_weights)

        # upcast attention to fp32
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)
        attn_output = torch.matmul(attn_weights, value_states)

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, -1)

        attn_output = self.o_proj(attn_output)

        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value


class Qwen2VLFlashAttention2(Qwen2VLAttention):
    """
    Qwen2VL flash attention module, following Qwen2VL attention module. This module inherits from `Qwen2VLAttention`
    as the weights of the module stays untouched. The only required change would be on the forward pass
    where it needs to correctly call the public API of flash attention and deal with padding tokens
    in case the input contains any of them. Additionally, for sliding window attention, we apply SWA only to the bottom
    config.max_window_layers layers.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.
        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.
        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).
        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
    ):
        bsz, q_len, _ = hidden_states.size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)

        # Because the input can be padded, the absolute sequence length depends on the max position id.
        cos, sin = position_embeddings
        query_states, key_states = apply_multimodal_rotary_pos_emb(
            query_states, key_states, cos, sin, self.rope_scaling["mrope_section"]
        )

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)
        dropout_rate = 0.0 if not self.training else self.attention_dropout

        # In PEFT, usually we cast the layer norms in float32 for training stability reasons
        # therefore the input hidden states gets silently casted in float32. Hence, we need
        # cast them back in float16 just to be sure everything works as expected.
        input_dtype = query_states.dtype
        if input_dtype == torch.float32:
            if torch.is_autocast_enabled():
                target_dtype = torch.get_autocast_gpu_dtype()
            # Handle the case where the model is quantized
            elif hasattr(self.config, "_pre_quantization_dtype"):
                target_dtype = self.config._pre_quantization_dtype
            else:
                target_dtype = self.q_proj.weight.dtype

            logger.warning_once(
                f"The input hidden states seems to be silently casted in float32, this might be related to"
                f" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in"
                f" {target_dtype}."
            )

            query_states = query_states.to(target_dtype)
            key_states = key_states.to(target_dtype)
            value_states = value_states.to(target_dtype)

        # Reashape to the expected shape for Flash Attention
        query_states = query_states.transpose(1, 2)
        key_states = key_states.transpose(1, 2)
        value_states = value_states.transpose(1, 2)

        if (
            self.config.use_sliding_window
            and getattr(self.config, "sliding_window", None) is not None
            and self.layer_idx >= self.config.max_window_layers
        ):
            sliding_window = self.config.sliding_window
        else:
            sliding_window = None

        attn_output = _flash_attention_forward(
            query_states,
            key_states,
            value_states,
            attention_mask,
            q_len,
            dropout=dropout_rate,
            sliding_window=sliding_window,
            is_causal=self.is_causal,
            use_top_left_mask=self._flash_attn_uses_top_left_mask,
        )

        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()
        attn_output = self.o_proj(attn_output)

        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value


class Qwen2VLSdpaAttention(Qwen2VLAttention):
    """
    Qwen2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from
    `Qwen2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to
    SDPA API.
    """

    # Adapted from Qwen2Attention.forward
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        if output_attentions:
            # TODO: Improve this warning with e.g. `model.config.attn_implementation = "manual"` once this is implemented.
            logger.warning_once(
                "Qwen2VLModel is using Qwen2VLSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, "
                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
            )
            return super().forward(
                hidden_states=hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
            )

        bsz, q_len, _ = hidden_states.size()

        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_multimodal_rotary_pos_emb(
            query_states, key_states, cos, sin, self.rope_scaling["mrope_section"]
        )

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        causal_mask = attention_mask
        if attention_mask is not None:  # no matter the length, we just slice it
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]

        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,
        # Reference: https://github.com/pytorch/pytorch/issues/112577.
        if query_states.device.type == "cuda" and attention_mask is not None:
            query_states = query_states.contiguous()
            key_states = key_states.contiguous()
            value_states = value_states.contiguous()

        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment
        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.
        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.
        is_causal = True if causal_mask is None and q_len > 1 else False

        attn_output = torch.nn.functional.scaled_dot_product_attention(
            query_states,
            key_states,
            value_states,
            attn_mask=causal_mask,
            dropout_p=self.attention_dropout if self.training else 0.0,
            is_causal=is_causal,
        )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(bsz, q_len, self.hidden_size)

        attn_output = self.o_proj(attn_output)

        return attn_output, None, past_key_value


QWEN2_VL_ATTENTION_CLASSES = {
    "eager": Qwen2VLAttention,
    "flash_attention_2": Qwen2VLFlashAttention2,
    "sdpa": Qwen2VLSdpaAttention,
}


class Qwen2VLDecoderLayer(nn.Module):
    def __init__(self, config: Qwen2VLConfig, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        if config.use_sliding_window and config._attn_implementation != "flash_attention_2":
            logger.warning_once(
                f"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; "
                "unexpected results may be encountered."
            )
        self.self_attn = QWEN2_VL_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)

        self.mlp = Qwen2MLP(config)
        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
        **kwargs,
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size
                `(batch, sequence_length)` where padding elements are indicated by 0.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            use_cache (`bool`, *optional*):
                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding
                (see `past_key_values`).
            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states
            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
                Indices depicting the position of the input sequence tokens in the sequence.
            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):
                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,
                with `head_dim` being the embedding dimension of each attention head.
            kwargs (`dict`, *optional*):
                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code
                into the model
        """

        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights,)

        if use_cache:
            outputs += (present_key_value,)

        return outputs


QWEN2VL_START_DOCSTRING = r"""
    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Parameters:
        config ([`Qwen2VLConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
"""


@add_start_docstrings(
    "The bare Qwen2VL Model outputting raw hidden-states without any specific head on top.",
    QWEN2VL_START_DOCSTRING,
)
class Qwen2VLPreTrainedModel(PreTrainedModel):
    config_class = Qwen2VLConfig
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen2VLDecoderLayer", "Qwen2VLVisionBlock"]
    _skip_keys_device_placement = "past_key_values"
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _supports_cache_class = True
    _supports_static_cache = False  # TODO (joao): fix. torch.compile failing probably due to `cache_positions`

    def _init_weights(self, module):
        std = self.config.initializer_range
        if isinstance(module, (nn.Linear, nn.Conv3d)):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()


class Qwen2VisionTransformerPretrainedModel(Qwen2VLPreTrainedModel):
    config_class = Qwen2VLVisionConfig
    _no_split_modules = ["Qwen2VLVisionBlock"]

    def __init__(self, config) -> None:
        super().__init__(config)
        self.spatial_merge_size = config.spatial_merge_size

        self.patch_embed = PatchEmbed(
            patch_size=config.patch_size,
            temporal_patch_size=config.temporal_patch_size,
            in_channels=config.in_channels,
            embed_dim=config.embed_dim,
        )

        head_dim = config.embed_dim // config.num_heads
        self.rotary_pos_emb = VisionRotaryEmbedding(head_dim // 2)

        self.blocks = nn.ModuleList(
            [Qwen2VLVisionBlock(config, config._attn_implementation) for _ in range(config.depth)]
        )
        self.merger = PatchMerger(
            dim=config.hidden_size, context_dim=config.embed_dim, spatial_merge_size=config.spatial_merge_size
        )
        self.gradient_checkpointing = False

    def get_dtype(self) -> torch.dtype:
        return self.blocks[0].mlp.fc2.weight.dtype

    def get_device(self) -> torch.device:
        return self.blocks[0].mlp.fc2.weight.device

    def rot_pos_emb(self, grid_thw):
        pos_ids = []
        for t, h, w in grid_thw:
            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
            hpos_ids = hpos_ids.reshape(
                h // self.spatial_merge_size,
                self.spatial_merge_size,
                w // self.spatial_merge_size,
                self.spatial_merge_size,
            )
            hpos_ids = hpos_ids.permute(0, 2, 1, 3)
            hpos_ids = hpos_ids.flatten()

            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
            wpos_ids = wpos_ids.reshape(
                h // self.spatial_merge_size,
                self.spatial_merge_size,
                w // self.spatial_merge_size,
                self.spatial_merge_size,
            )
            wpos_ids = wpos_ids.permute(0, 2, 1, 3)
            wpos_ids = wpos_ids.flatten()
            pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
        pos_ids = torch.cat(pos_ids, dim=0)
        max_grid_size = grid_thw[:, 1:].max()
        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
        return rotary_pos_emb

    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:
        hidden_states = self.patch_embed(hidden_states)
        rotary_pos_emb = self.rot_pos_emb(grid_thw)
        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)
        position_embeddings = (emb.cos(), emb.sin())

        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(
            dim=0,
            # Select dtype based on the following factors:
            #  - FA2 requires that cu_seqlens_q must have dtype int32
            #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw
            # See https://github.com/huggingface/transformers/pull/34852 for more information
            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,
        )
        cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)

        for blk in self.blocks:
            if self.gradient_checkpointing and self.training:
                hidden_states = self._gradient_checkpointing_func(
                    blk.__call__, hidden_states, cu_seqlens, None, position_embeddings
                )
            else:
                hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, position_embeddings=position_embeddings)

        return self.merger(hidden_states)


@add_start_docstrings(
    "The bare Qwen2VL Model outputting raw hidden-states without any specific head on top.",
    QWEN2VL_START_DOCSTRING,
)
class Qwen2VLModel(Qwen2VLPreTrainedModel):
    def __init__(self, config: Qwen2VLConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [Qwen2VLDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self._attn_implementation = config._attn_implementation
        self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen2VLRotaryEmbedding(config=config)

        self.gradient_checkpointing = False
        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training:
            if use_cache:
                logger.warning_once(
                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                )
                use_cache = False

        # torch.jit.trace() doesn't support cache objects in the output
        if use_cache and past_key_values is None and not torch.jit.is_tracing():
            past_key_values = DynamicCache()

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        # the hard coded `3` is for temporal, height and width.
        if position_ids is None:
            position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)
        elif position_ids.dim() == 2:
            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)

        causal_mask = self._update_causal_mask(
            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions
        )

        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        next_decoder_cache = None

        for decoder_layer in self.layers:
            if output_hidden_states:
                all_hidden_states += (hidden_states,)

            if self.gradient_checkpointing and self.training:
                layer_outputs = self._gradient_checkpointing_func(
                    decoder_layer.__call__,
                    hidden_states,
                    causal_mask,
                    position_ids,
                    past_key_values,
                    output_attentions,
                    use_cache,
                    cache_position,
                    position_embeddings,
                )
            else:
                layer_outputs = decoder_layer(
                    hidden_states,
                    attention_mask=causal_mask,
                    position_ids=position_ids,
                    past_key_value=past_key_values,
                    output_attentions=output_attentions,
                    use_cache=use_cache,
                    cache_position=cache_position,
                    position_embeddings=position_embeddings,
                )

            hidden_states = layer_outputs[0]

            if use_cache:
                next_decoder_cache = layer_outputs[2 if output_attentions else 1]

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        # add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        next_cache = next_decoder_cache if use_cache else None

        if not return_dict:
            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=next_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )

    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Qwen2VL
    def _update_causal_mask(
        self,
        attention_mask: torch.Tensor,
        input_tensor: torch.Tensor,
        cache_position: torch.Tensor,
        past_key_values: Cache,
        output_attentions: bool = False,
    ):
        if self.config._attn_implementation == "flash_attention_2":
            if attention_mask is not None and past_key_values is not None:
                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]
                if is_padding_right:
                    raise ValueError(
                        "You are attempting to perform batched generation with padding_side='right'"
                        " this may lead to unexpected behaviour for Flash Attention version of Qwen2VL. Make sure to "
                        " call `tokenizer.padding_side  = 'left'` before tokenizing the input. "
                    )
            if attention_mask is not None and 0.0 in attention_mask:
                return attention_mask
            return None

        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in
        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail
        # to infer the attention mask.
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        using_static_cache = isinstance(past_key_values, StaticCache)
        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)

        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward
        if (
            self.config._attn_implementation == "sdpa"
            and not (using_static_cache or using_sliding_window_cache)
            and not output_attentions
        ):
            if AttentionMaskConverter._ignore_causal_mask_sdpa(
                attention_mask,
                inputs_embeds=input_tensor,
                past_key_values_length=past_seen_tokens,
                sliding_window=self.config.sliding_window,
                is_training=self.training,
            ):
                return None

        dtype, device = input_tensor.dtype, input_tensor.device
        min_dtype = torch.finfo(dtype).min
        sequence_length = input_tensor.shape[1]
        # SlidingWindowCache or StaticCache
        if using_sliding_window_cache or using_static_cache:
            target_length = past_key_values.get_max_cache_shape()
        # DynamicCache or no cache
        else:
            target_length = (
                attention_mask.shape[-1]
                if isinstance(attention_mask, torch.Tensor)
                else past_seen_tokens + sequence_length + 1
            )

        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).
        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(
            attention_mask,
            sequence_length=sequence_length,
            target_length=target_length,
            dtype=dtype,
            device=device,
            cache_position=cache_position,
            batch_size=input_tensor.shape[0],
            config=self.config,
            past_key_values=past_key_values,
        )

        if (
            self.config._attn_implementation == "sdpa"
            and attention_mask is not None
            and attention_mask.device.type in ["cuda", "xpu"]
            and not output_attentions
        ):
            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when
            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.
            # Details: https://github.com/pytorch/pytorch/issues/110213
            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)

        return causal_mask

    @staticmethod
    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Qwen2VL
    def _prepare_4d_causal_attention_mask_with_cache_position(
        attention_mask: torch.Tensor,
        sequence_length: int,
        target_length: int,
        dtype: torch.dtype,
        device: torch.device,
        cache_position: torch.Tensor,
        batch_size: int,
        config: Qwen2VLConfig,
        past_key_values: Cache,
    ):
        """
        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape
        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.

        Args:
            attention_mask (`torch.Tensor`):
                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.
            sequence_length (`int`):
                The sequence length being processed.
            target_length (`int`):
                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.
            dtype (`torch.dtype`):
                The dtype to use for the 4D attention mask.
            device (`torch.device`):
                The device to place the 4D attention mask on.
            cache_position (`torch.Tensor`):
                Indices depicting the position of the input sequence tokens in the sequence.
            batch_size (`torch.Tensor`):
                Batch size.
            config (`Qwen2VLConfig`):
                The model's configuration class
            past_key_values (`Cache`):
                The cache class that is being used currently to generate
        """
        if attention_mask is not None and attention_mask.dim() == 4:
            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.
            causal_mask = attention_mask
        else:
            min_dtype = torch.finfo(dtype).min
            causal_mask = torch.full(
                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device
            )
            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)
            if config.sliding_window is not None:
                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also
                # the check is needed to verify is current checkpoint was trained with sliding window or not
                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:
                    sliding_attend_mask = torch.arange(target_length, device=device) <= (
                        cache_position.reshape(-1, 1) - config.sliding_window
                    )
                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)
            causal_mask *= diagonal_attend_mask
            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)
            if attention_mask is not None:
                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit
                if attention_mask.shape[-1] > target_length:
                    attention_mask = attention_mask[:, :target_length]
                mask_length = attention_mask.shape[-1]
                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(
                    causal_mask.device
                )
                padding_mask = padding_mask == 0
                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(
                    padding_mask, min_dtype
                )
        return causal_mask


QWEN2_VL_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
            it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see
            `past_key_values`).

            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape
            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.

            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that
            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all
            `decoder_input_ids` of shape `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
            model's internal embedding lookup matrix.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
            `past_key_values`).
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
            tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
            more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        pixel_values (`torch.FloatTensor` of shape `(seq_length, num_channels * image_size * image_size)):
            The tensors corresponding to the input images. Pixel values can be obtained using
            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2VLProcessor`] uses
            [`Qwen2VLImageProcessor`] for processing images.
        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):
            The tensors corresponding to the input videos. Pixel values can be obtained using
            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2VLProcessor`] uses
            [`Qwen2VLImageProcessor`] for processing videos.
        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
            The temporal, height and width of feature shape of each image in LLM.
        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
            The temporal, height and width of feature shape of each video in LLM.
        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):
            The rope index difference between sequence length and multimodal rope.
"""


class Qwen2VLForConditionalGeneration(Qwen2VLPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config):
        super().__init__(config)
        self.visual = Qwen2VisionTransformerPretrainedModel._from_config(config.vision_config)
        self.model = Qwen2VLModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.rope_deltas = None  # cache rope_deltas here

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    def get_rope_index(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Calculate the 3D rope index based on image and video's temporal, height and width in LLM.

        Explanation:
            Each embedding sequence contains vision embedding and text embedding or just contains text embedding.

            For pure text embedding sequence, the rotary position embedding has no difference with modern LLMs.
            Examples:
                input_ids: [T T T T T], here T is for text.
                temporal position_ids: [0, 1, 2, 3, 4]
                height position_ids: [0, 1, 2, 3, 4]
                width position_ids: [0, 1, 2, 3, 4]

            For vision and text embedding sequence, we calculate 3D rotary position embedding for vision part
            and 1D rotary position embedding for text part.
            Examples:
                Assume we have a video input with 3 temporal patches, 2 height patches and 2 width patches.
                input_ids: [V V V V V V V V V V V V T T T T T], here V is for vision.
                vision temporal position_ids: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]
                vision height position_ids: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]
                vision width position_ids: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
                text temporal position_ids: [3, 4, 5, 6, 7]
                text height position_ids: [3, 4, 5, 6, 7]
                text width position_ids: [3, 4, 5, 6, 7]
                Here we calculate the text start position_ids as the max vision position_ids plus 1.

        Args:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
                it.
            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):
                The temporal, height and width of feature shape of each image in LLM.
            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):
                The temporal, height and width of feature shape of each video in LLM.
            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

                - 1 for tokens that are **not masked**,
                - 0 for tokens that are **masked**.

        Returns:
            position_ids (`torch.LongTensor` of shape `(3, batch_size, sequence_length)`)
            mrope_position_deltas (`torch.Tensor` of shape `(batch_size)`)
        """
        spatial_merge_size = self.config.vision_config.spatial_merge_size
        image_token_id = self.config.image_token_id
        video_token_id = self.config.video_token_id
        vision_start_token_id = self.config.vision_start_token_id
        mrope_position_deltas = []
        if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):
            total_input_ids = input_ids
            if attention_mask is None:
                attention_mask = torch.ones_like(total_input_ids)
            position_ids = torch.ones(
                3, input_ids.shape[0], input_ids.shape[1], dtype=input_ids.dtype, device=input_ids.device
            )
            image_index, video_index = 0, 0
            for i, input_ids in enumerate(total_input_ids):
                input_ids = input_ids[attention_mask[i].to(input_ids.device) == 1]
                image_nums, video_nums = 0, 0
                vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)
                vision_tokens = input_ids[vision_start_indices + 1]
                image_nums = (vision_tokens == image_token_id).sum()
                video_nums = (vision_tokens == video_token_id).sum()
                input_tokens = input_ids.tolist()
                llm_pos_ids_list: list = []
                st = 0
                remain_images, remain_videos = image_nums, video_nums
                for _ in range(image_nums + video_nums):
                    if image_token_id in input_tokens and remain_images > 0:
                        ed_image = input_tokens.index(image_token_id, st)
                    else:
                        ed_image = len(input_tokens) + 1
                    if video_token_id in input_tokens and remain_videos > 0:
                        ed_video = input_tokens.index(video_token_id, st)
                    else:
                        ed_video = len(input_tokens) + 1
                    if ed_image < ed_video:
                        t, h, w = (
                            image_grid_thw[image_index][0],
                            image_grid_thw[image_index][1],
                            image_grid_thw[image_index][2],
                        )
                        image_index += 1
                        remain_images -= 1
                        ed = ed_image
                    else:
                        t, h, w = (
                            video_grid_thw[video_index][0],
                            video_grid_thw[video_index][1],
                            video_grid_thw[video_index][2],
                        )
                        video_index += 1
                        remain_videos -= 1
                        ed = ed_video
                    llm_grid_t, llm_grid_h, llm_grid_w = (
                        t.item(),
                        h.item() // spatial_merge_size,
                        w.item() // spatial_merge_size,
                    )
                    text_len = ed - st

                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)

                    t_index = torch.arange(llm_grid_t).view(-1, 1).expand(-1, llm_grid_h * llm_grid_w).flatten()
                    h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(llm_grid_t, -1, llm_grid_w).flatten()
                    w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(llm_grid_t, llm_grid_h, -1).flatten()
                    llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + text_len + st_idx)
                    st = ed + llm_grid_t * llm_grid_h * llm_grid_w

                if st < len(input_tokens):
                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
                    text_len = len(input_tokens) - st
                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)

                llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)
                mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))
            mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)
            return position_ids, mrope_position_deltas
        else:
            if attention_mask is not None:
                position_ids = attention_mask.long().cumsum(-1) - 1
                position_ids.masked_fill_(attention_mask == 0, 1)
                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(attention_mask.device)
                max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]
                mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]
            else:
                position_ids = (
                    torch.arange(input_ids.shape[1], device=input_ids.device)
                    .view(1, 1, -1)
                    .expand(3, input_ids.shape[0], -1)
                )
                mrope_position_deltas = torch.zeros(
                    [input_ids.shape[0], 1],
                    device=input_ids.device,
                    dtype=input_ids.dtype,
                )

            return position_ids, mrope_position_deltas

    @add_start_docstrings_to_model_forward(QWEN2_VL_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=Qwen2VLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        pixel_values: Optional[torch.Tensor] = None,
        pixel_values_videos: Optional[torch.FloatTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        rope_deltas: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Union[Tuple, Qwen2VLCausalLMOutputWithPast]:
        r"""
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

        Returns:

        Example:

        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, Qwen2VLForConditionalGeneration

        >>> model = Qwen2VLForConditionalGeneration.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")
        >>> processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")

        >>> messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "What is shown in this image?"},
                ],
            },
        ]
        >>> url = "https://www.ilankelman.org/stopsigns/australia.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ..."
        ```"""

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if inputs_embeds is None:
            inputs_embeds = self.model.embed_tokens(input_ids)
            if pixel_values is not None:
                pixel_values = pixel_values.type(self.visual.get_dtype())
                image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)
                n_image_tokens = (input_ids == self.config.image_token_id).sum().item()
                n_image_features = image_embeds.shape[0]
                if n_image_tokens != n_image_features:
                    raise ValueError(
                        f"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}"
                    )
                image_mask = (
                    (input_ids == self.config.image_token_id)
                    .unsqueeze(-1)
                    .expand_as(inputs_embeds)
                    .to(inputs_embeds.device)
                )
                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)
                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)

            if pixel_values_videos is not None:
                pixel_values_videos = pixel_values_videos.type(self.visual.get_dtype())
                video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)
                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()
                n_video_features = video_embeds.shape[0]
                if n_video_tokens != n_video_features:
                    raise ValueError(
                        f"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}"
                    )
                video_mask = (
                    (input_ids == self.config.video_token_id)
                    .unsqueeze(-1)
                    .expand_as(inputs_embeds)
                    .to(inputs_embeds.device)
                )
                video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)
                inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)

            if attention_mask is not None:
                attention_mask = attention_mask.to(inputs_embeds.device)

        # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme
        if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):
            # calculate RoPE index once per generation in the pre-fill stage only
            if (
                (cache_position is not None and cache_position[0] == 0)
                or self.rope_deltas is None
                or (past_key_values is None or past_key_values.get_seq_length() == 0)
            ):
                position_ids, rope_deltas = self.get_rope_index(
                    input_ids, image_grid_thw, video_grid_thw, attention_mask
                )
                self.rope_deltas = rope_deltas
            # then use the prev pre-calculated rope-deltas to get the correct position ids
            else:
                batch_size, seq_length, _ = inputs_embeds.shape
                delta = cache_position[0] + self.rope_deltas if cache_position is not None else 0
                position_ids = torch.arange(seq_length, device=inputs_embeds.device)
                position_ids = position_ids.view(1, -1).expand(batch_size, -1)
                if cache_position is not None:  # otherwise `deltas` is an int `0`
                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)
                    delta = delta.to(position_ids.device)
                position_ids = position_ids.add(delta)
                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)

        outputs = self.model(
            input_ids=None,
            position_ids=position_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )

        hidden_states = outputs[0]
        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            # Upcast to float if we need to compute the loss to avoid potential precision issues
            logits = logits.float()
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)
            loss = loss_fct(shift_logits, shift_labels)

        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return Qwen2VLCausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            rope_deltas=self.rope_deltas,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        attention_mask=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        use_cache=True,
        pixel_values=None,
        pixel_values_videos=None,
        image_grid_thw=None,
        video_grid_thw=None,
        **kwargs,
    ):
        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model

        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            position_ids=position_ids,
            pixel_values=pixel_values,
            pixel_values_videos=pixel_values_videos,
            image_grid_thw=image_grid_thw,
            video_grid_thw=video_grid_thw,
            use_cache=use_cache,
            **kwargs,
        )

        # Qwen2-VL position_ids are prepareed with rope_deltas in forward
        model_inputs["position_ids"] = None

        if model_inputs["cache_position"][0] != 0:
            model_inputs["pixel_values"] = None
            model_inputs["pixel_values_videos"] = None

        return model_inputs

    def _get_image_nums_and_video_nums(
        self,
        input_ids: Optional[torch.LongTensor],
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get the number of images and videos for each sample to calculate the separation length of the sample tensor.
        These parameters are not passed through the processor to avoid unpredictable impacts from interface modifications.

        Args:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                Indices of input sequence tokens in the vocabulary.

        Returns:
            image_nums (`torch.LongTensor` of shape `(batch_size, num_images_sample)`)
            video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)
        """
        image_token_id = self.config.image_token_id
        video_token_id = self.config.video_token_id
        vision_start_token_id = self.config.vision_start_token_id

        vision_start_mask = input_ids == vision_start_token_id
        vision_first_mask = torch.roll(vision_start_mask, shifts=1, dims=1)
        image_mask = input_ids == image_token_id
        video_mask = input_ids == video_token_id
        image_nums = torch.sum(vision_first_mask & image_mask, dim=1)
        video_nums = torch.sum(vision_first_mask & video_mask, dim=1)

        return image_nums, video_nums

    def _expand_inputs_for_generation(
        self,
        expand_size: int = 1,
        is_encoder_decoder: bool = False,
        input_ids: Optional[torch.LongTensor] = None,
        **model_kwargs,
    ) -> Tuple[torch.LongTensor, Dict[str, Any]]:
        # Overwritten -- Support for expanding tensors without a batch size dimension
        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw, second_per_grid_t
        # pixel_values.shape[0] is sum(seqlen_images for samples)
        # image_grid_thw.shape[0] is sum(num_images for samples)

        if expand_size == 1:
            return input_ids, model_kwargs

        visual_keys = ["pixel_values", "image_grid_thw", "pixel_values_videos", "video_grid_thw", "second_per_grid_ts"]

        def _expand_dict_for_generation_visual(dict_to_expand):
            image_grid_thw = model_kwargs.get("image_grid_thw", None)
            video_grid_thw = model_kwargs.get("video_grid_thw", None)
            image_nums, video_nums = self._get_image_nums_and_video_nums(input_ids)

            def _repeat_interleave_samples(x, lengths, repeat_times):
                samples = torch.split(x, lengths)
                repeat_args = [repeat_times] + [1] * (x.dim() - 1)
                result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)
                return result

            for key in dict_to_expand:
                if key == "pixel_values":
                    # split images into samples
                    samples = torch.split(image_grid_thw, list(image_nums))
                    # compute the sequence length of images for each sample
                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "image_grid_thw":
                    # get the num of images for each sample
                    lengths = list(image_nums)
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "pixel_values_videos":
                    samples = torch.split(video_grid_thw, list(video_nums))
                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "video_grid_thw":
                    lengths = list(video_nums)
                    dict_to_expand[key] = _repeat_interleave_samples(
                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size
                    )
                elif key == "second_per_grid_ts":
                    if not isinstance(dict_to_expand[key], list):
                        raise TypeError(
                            f"Expected value for key '{key}' to be a list, but got {type(dict_to_expand[key])} instead."
                        )
                    tensor = torch.tensor(dict_to_expand[key])
                    lengths = list(video_nums)
                    tensor = _repeat_interleave_samples(tensor, lengths=lengths, repeat_times=expand_size)
                    dict_to_expand[key] = tensor.tolist()
            return dict_to_expand

        def _expand_dict_for_generation(dict_to_expand):
            for key in dict_to_expand:
                if (
                    key != "cache_position"
                    and dict_to_expand[key] is not None
                    and isinstance(dict_to_expand[key], torch.Tensor)
                    and key not in visual_keys
                ):
                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)
            return dict_to_expand

        # input_ids is required for expanding visual inputs
        # If input_ids is unavailable, visual inputs will not be used; therefore, there is no need to expand visual inputs.
        if input_ids is not None and input_ids.numel() != 0:
            model_kwargs = _expand_dict_for_generation_visual(model_kwargs)

        if input_ids is not None:
            input_ids = input_ids.repeat_interleave(expand_size, dim=0)

        model_kwargs = _expand_dict_for_generation(model_kwargs)

        if is_encoder_decoder:
            if model_kwargs.get("encoder_outputs") is None:
                raise ValueError("If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.")
            model_kwargs["encoder_outputs"] = _expand_dict_for_generation(model_kwargs["encoder_outputs"])

        return input_ids, model_kwargs


__all__ = ["Qwen2VLForConditionalGeneration", "Qwen2VLModel", "Qwen2VLPreTrainedModel"]


<reference code from original author>
# simple_infer.ipynb
# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.
#%%
import os
import sys
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
sys.path.append('../')

import re
import glob
import json
import random
import pickle
import numpy as np
from tqdm.auto import tqdm
from itertools import product

import cv2
from PIL import Image, ImageDraw
from matplotlib import pyplot as plt
import torch
#%%
from transformers import BitsAndBytesConfig, LogitsProcessor
from peft import PeftModel

from llava.model.builder import load_pretrained_model
from llava.mm_utils import process_images, tokenizer_image_token
from llava.constants import IMAGE_TOKEN_INDEX

from efficientvit.sam_model_zoo import create_sam_model
from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor
#%%
PRETRAINED = "lmms-lab/llama3-llava-next-8b"
ADAPTER_PATH = './checkpoint/sam4mllm/' # or './checkpoint/sam4mllm_plus/'
EFFVIT_SAM_PATH = "./checkpoint/xl1.pt"
#%%
def load_model():
    tokenizer, model, image_processor, max_length = load_pretrained_model(
        PRETRAINED,
        None,
        "llava_llama3",
        device_map='cuda:0',
        # for v100
        torch_dtype=torch.float16,
        attn_implementation='eager',

        # for a100 or later
        # torch_dtype=torch.bfloat16,
        # attn_implementation='flash_attention_2',
    )
    tokenizer.pad_token = tokenizer.eos_token
    model.generation_config.pad_token_id = tokenizer.pad_token_id

    model.eval()
    model.tie_weights()

    if ADAPTER_PATH:
        model = PeftModel.from_pretrained(model, ADAPTER_PATH)

    effvit_sam = create_sam_model(
        name="xl1", weight_url=EFFVIT_SAM_PATH,
    )


    effvit_sam = effvit_sam.cuda().eval()

    sd = torch.load('effvit_xl1_decoder_coco_ft.pt')
    sd = {k.replace("model.", ""): v for k, v in sd.items()}
    r = effvit_sam.load_state_dict(sd, strict=False)
    print('unexpected_keys:', r.unexpected_keys)

    effvit_sam_predictor = EfficientViTSamPredictor(effvit_sam)

    return model, tokenizer, image_processor, effvit_sam_predictor
#%%
model, tokenizer, image_processor, effvit_sam_predictor = load_model()
model = model.eval()
#%%
grid_rand_points = [
    (93, 70), (51, 89), (91, 90), (32, 32), (88, 10),
    (12, 28), (29, 52), (49, 49), (28, 12), (59, 60),
    (9, 48), (52, 29), (31, 92), (68, 13), (73, 73),
    (69, 53), (48, 9), (19, 18), (71, 93), (53, 69),
    (89, 50), (11, 88), (33, 72), (39, 41), (72, 33),
    (13, 68), (79, 82), (8, 8), (81, 22), (92, 30),
]
grid_rand_points = np.array(grid_rand_points) / 100
grid_rand_points = grid_rand_points[:15]
number_tokens = tokenizer(' '.join([str(i) for i in range(1000)]), add_special_tokens=False)['input_ids'][::2]
#%%
grid_rand_points
#%%
test_img_path = './test_imgs/000000025515.jpg'
image = Image.open(test_img_path)
#%%
answer_counts = '1'
s_phrase = 'side view bird'
device = 'cuda:0'
#%%
image_tensor = process_images([image], image_processor, model.config)
image_tensor = [_image.to(dtype=torch.bfloat16) for _image in image_tensor]
image_sizes = [image.size]
#%%
prompt_question = tokenizer.apply_chat_template([
    {"role": "system", "content": "You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language."},
    {"role": "user", "content": f'<image>\nPlease provide the bounding box coordinate of the region this sentence describes ({answer_counts}):\n"{s_phrase}".'},
], tokenize=False, add_generation_prompt=True)
input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt").unsqueeze(0).to(device)
#%%
model = model.merge_and_unload()
#%%
with torch.backends.cuda.sdp_kernel(enable_flash=False):
    output = model.generate(
        input_ids,
        images=[x.half() for x in image_tensor],
        image_sizes=image_sizes,
        max_new_tokens=30,
    )
#%%
text_output = tokenizer.decode(output[0], skip_special_tokens=True)
text_output
#%%
def point_to_str(point):
    return f"({point[0]:03d},{point[1]:03d})"
#%%
bbox = [281,240,863,999]
bbox_txt = '[281,240,863,999]'
x1, y1, x2, y2 = bbox

rand_points = grid_rand_points * np.array([x2 - x1, y2 - y1]) + np.array([x1, y1])
rand_points = rand_points.astype(int)

points_txt = ' '.join([point_to_str(p) for p in rand_points])
question_points = f'Check if the points listed below are located on the object with coordinates {bbox_txt}:\n{points_txt}'

prompt_question = tokenizer.apply_chat_template([
    {"role": "system", "content": "You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language."},
    {"role": "user", "content": f'<image>\nPlease provide the bounding box coordinate of the region this sentence describes ({answer_counts}):\n"{s_phrase}".'},
    {"role": "assistant", "content": text_output},
    {"role": "user", "content": question_points},
], tokenize=False, add_generation_prompt=True)
input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt").unsqueeze(0).to(device)
#%%
output_2 = model.generate(
    input_ids,
    images=[x.half() for x in image_tensor],
    image_sizes=image_sizes,
    max_new_tokens=30,
    output_logits=True,
    return_dict_in_generate=True,
)
#%%
output_2[0]
#%%
text_output_2 = tokenizer.decode(output_2[0][0], skip_special_tokens=True)
text_output_2
#%%
yesno_probs = torch.stack(output_2['logits'], dim=1).softmax(dim=-1)
yesno_probs = yesno_probs[0, :30, [2822, 9642]].float().cpu().numpy()
#%%
yesno_probs
#%%
def sel_points(rand_points, all_probs_2, neg_thres=0.2, pos_thres=0.8):
    sel_points, sel_labels = [], []
    for (x, y), score in zip(rand_points, all_probs_2):
        if score[0] > neg_thres:
            sel_points.append((x, y)), sel_labels.append(0)
        elif score[1] > pos_thres:
            sel_points.append((x, y)), sel_labels.append(1)

    sel_points, sel_labels = np.array(sel_points), np.array(sel_labels)

    return sel_points, sel_labels

points_sel, labels_sel = sel_points(rand_points, yesno_probs, neg_thres=0.9, pos_thres=0.75)
#%%
draw_image = image.copy()
draw = ImageDraw.Draw(draw_image)
img_w, img_h = image.size
draw.rectangle([x1/1000*img_w, y1/1000*img_h, x2/1000*img_w, y2/1000*img_h], outline='red', width=5)

for (x, y), label in zip(points_sel, labels_sel):
    draw.ellipse([x/1000*img_w-5, y/1000*img_h-5, x/1000*img_w+5, y/1000*img_h+5], fill='red' if label else 'blue')

draw_image
#%%
def sam_pred_mask(effvit_sam_predictor, points_sel, labels_sel, bbox, ori_img_w, ori_img_h):
    if len(points_sel) != 0:
        scaled_sel_points = points_sel / [1000, 1000] * [ori_img_w, ori_img_h]
    else:
        scaled_sel_points, labels_sel = None, None
    scaled_bbox = np.array(bbox) / 1000 * [ori_img_w, ori_img_h, ori_img_w, ori_img_h]
    pred_masks, scores, logits = effvit_sam_predictor.predict(
        point_coords=scaled_sel_points,
        point_labels=labels_sel,
        box=scaled_bbox[None, :],
        multimask_output=True,
    )
    pred_mask = pred_masks[scores.argmax()]

    return pred_mask
#%%
points_sel
#%%
labels_sel
#%%
bbox
#%%
img_w
#%%
effvit_sam_predictor.set_image(np.array(image))
pred_mask = sam_pred_mask(
    effvit_sam_predictor, points_sel, labels_sel, bbox, img_w, img_h
)
#%%
plt.imshow(image)
plt.imshow(pred_mask, alpha=0.5)
plt.show()


# to_chat_format.ipynb
# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.
#%%
import orjson
from transformers import AutoTokenizer

import os
import cv2
from copy import deepcopy
import random
import numpy as np
from PIL import Image
from matplotlib import pyplot as plt
from collections import defaultdict, Counter
from tqdm import tqdm
#%%
PRETRAINED = "lmms-lab/llama3-llava-next-8b"
SYSTEM_PROMPT = "You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language."
#%%
tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)
#%%
all_data = []
with open('../../data/processed_data_v2/refcoco_data.json', 'r') as f:
    all_data += orjson.loads(f.read())

# with open('../../data/processed_data_v2/ade20k_ref_data.json', 'r') as f:
#     all_data += orjson.loads(f.read())

# with open('../../data/processed_data_v2/paco_ref_data.json', 'r') as f:
#     all_data += orjson.loads(f.read())

# with open('../../data/processed_data_v2/partimagenet_ref_data.json', 'r') as f:
#     all_data += orjson.loads(f.read())
#%%
img_grouped_data = {}
for d in all_data:
    img_grouped_data.setdefault(d['image_path'], []).append(d)

img_bboxs = {}
for d in all_data:
    if len(d['bboxes']) > 0:
        img_bboxs.setdefault(d['image_path'], []).append(tuple(d['bboxes'][0]))
#%%
len(img_grouped_data), len(img_bboxs)
#%%
sample_group = list(img_grouped_data.values())[777]
d = sample_group[10]
len(sample_group)
#%%
[x['phrases'] for x in sample_group]
#%%
count_dict = {
    '1': 'single answer',
    '1+': 'maybe multiple answers',
    '0+': 'maybe no or multiple answers',
}
#%%
def bbox_to_str(bbox):
    return f"[{bbox[0]:03d},{bbox[1]:03d},{bbox[2]:03d},{bbox[3]:03d}]"

def point_to_str(point):
    return f"({point[0]:03d},{point[1]:03d})"
#%%
MAX_PACKING = 5
all_convs = []

for img_path, sample_group in tqdm(img_grouped_data.items()):

    sample_group_copy = deepcopy(sample_group)
    random.shuffle(sample_group_copy)

    to_i_sample = 0
    for _ in range(20):
        if to_i_sample >= len(sample_group_copy):
            break

        img_conv = [{"role": "system", "content": SYSTEM_PROMPT}]

        for i_conv, i_sample in enumerate(range(to_i_sample, to_i_sample+MAX_PACKING)):
            if i_sample >= len(sample_group_copy):
                break

            ref_sample = sample_group_copy[i_sample]

            ref_conv = []
            if isinstance(ref_sample['phrases'], list):
                s_phrase = random.choice(ref_sample['phrases'])
            else:
                s_phrase = ref_sample['phrases']

            # print(s_phrase)

            # answer_counts = ref_sample['answer_counts']
            # answer_counts_str = count_dict[answer_counts]

            bboxes = np.array(ref_sample['bboxes'])
            points_and_labels = ref_sample['points_and_labels']

            answer_counts_str = '0+'
            question_box = '<image>\n' if i_conv == 0 else ''
            question_box += f'Please provide the bounding box coordinate of the region this sentence describes ({answer_counts_str}):\n"{s_phrase}".'
            if len(bboxes) == 0:
                answer_box = 'No object found.'
            else:
                answer_box = ' '.join([bbox_to_str(x) for x in bboxes])

            ref_conv.extend([
                {"role": "user", "content": question_box},
                {"role": "assistant", "content": f'\n{answer_box}'}
            ])

            bb_pnls = list(zip(bboxes, points_and_labels))
            random.shuffle(bb_pnls)
            for bbox, p_n_ls in bb_pnls:
                n_sel_points = random.normalvariate(10, 4)
                n_sel_points = int(max(1, min(20, n_sel_points)))
                # print('n_sel_points', n_sel_points)
                sampled_points_and_labels = random.sample(p_n_ls, n_sel_points)

                points_txt = ' '.join([point_to_str(x[:2]) for x in sampled_points_and_labels])
                question_points = 'Check if the points listed below are located on the object with bounding box {}:\n{}'.format(
                    bbox_to_str(bbox), points_txt)
                answer_points = ''.join(['Yes' if x[2] else 'No' for x in sampled_points_and_labels])

                ref_conv.extend([
                    {"role": "user", "content": question_points},
                    {"role": "assistant", "content": f'\n{answer_points}'}
                ])

            test_input_ids = tokenizer.apply_chat_template(img_conv + ref_conv, tokenize=True)
            # print(len(test_input_ids))
            if len(test_input_ids) > 1536:
                # print('fulled! go next\n')
                break
            else:
                img_conv.extend(ref_conv)
                to_i_sample = i_sample + 1


        all_convs.append({
            'image_path': img_path,
            'conversation': img_conv
        })
#%%
with open('./refcoco_convs_ep1.json', 'w') as f:
    f.write(orjson.dumps(all_convs).decode())
#%%


# /home/dbcloud/PycharmProjects/mllm4sam/SAM4MLLM/sam4mllm_train.py
# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.


# %%
import os
import sys
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128,garbage_collection_threshold:0.9"

import orjson
import warnings
from itertools import chain
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import gc
from tqdm.auto import tqdm
from PIL import Image
from wandb.integration.lightning.fabric import WandbLogger

import torch
from lightning.fabric import Fabric

from llava.mm_utils import process_images, tokenizer_image_token
from llava.constants import IMAGE_TOKEN_INDEX

# %%
PRETRAINED = "/root/autodl-tmp/big_models/llama3-llava-next-8b"

NUM_EPOCH = 3
BATCH_SIZE = 1
GRAD_ACC_STEPS = 8
MAX_LEN = 1536
USE_WANDB = True

RESUME = False
RESUME_PATH='./ckp/checkpoint-latest'
# %%
class LlavaDataset(torch.utils.data.Dataset):
    def __init__(self, data, tokenizer,
                image_processor, model_config,
                max_len=1536,
            img_dir='/home/ai2lab/work/datasets/',
            img_size=(672,672),
        ):
        self.data = data
        self.tokenizer = tokenizer
        self.image_processor = image_processor
        self.model_config = model_config
        self.max_len = max_len
        self.img_dir = img_dir
        self.img_size = img_size

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx, tokenize=True):
        d = self.data[idx]
        image_path = d['image_path']
        image = Image.open(self.img_dir + image_path).resize(self.img_size)

        conv_text = self.tokenizer.apply_chat_template(d['conversation'], tokenize=False)

        if tokenize:
            input_ids = tokenizer_image_token(conv_text, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt")
            input_ids = input_ids[:self.max_len]
        else:
            input_ids = conv_text

        pixel_values = process_images([image], self.image_processor, self.model_config)[0]
        image_sizes = list(self.img_size)

        sample = {
            'input_ids': input_ids,
            'images': pixel_values,
            'image_sizes': image_sizes
        }

        return sample

def load_processor():
    from transformers import AutoConfig, AutoTokenizer, AutoImageProcessor

    model_config = AutoConfig.from_pretrained(PRETRAINED)
    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED, use_fast=False)
    tokenizer.pad_token = tokenizer.eos_token
    image_processor = AutoImageProcessor.from_pretrained(PRETRAINED)

    return tokenizer, image_processor, model_config

def load_dataset(tokenizer, image_processor, model_config, max_len=1536):
    from data import GroupRefDataset

    with open('./refcoco_convs/refcoco_convs_ep1.json', 'r') as f:
        refcoco_data = orjson.loads(f.read())

    all_data = refcoco_data

    train_dataset = LlavaDataset(
        all_data,
        tokenizer,
        image_processor,
        model_config,
        max_len=max_len,
        img_dir='/root/autodl-tmp/img_datasets/',
        img_size=(672, 672),
    )
    print(f'num train examples: {len(train_dataset)}')

    return train_dataset

def load_model():
    import torch
    from llava.model.builder import load_pretrained_model
    from llava.model import LlavaLlamaForCausalLM

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        _, model, _, max_length = load_pretrained_model(
            PRETRAINED,
            None,
            "llava_llama3",
            # device_map=device_map,
            # device_map="auto",
            device_map=None,
            torch_dtype=torch.bfloat16,
            attn_implementation='flash_attention_2',
            # quantization_config=quantization_config,
        )

    model.eval()
    model.tie_weights()

    return model

def load_lora_model(model):
    from peft import PeftModel, PeftConfig
    if RESUME_PATH:
        model = PeftModel.from_pretrained(model, RESUME_PATH, is_trainable=True)

    return model

def get_lora_model(model):
    from peft import LoraConfig, get_peft_model

    target_modules = []
    for i in range(32):
        target_modules.append(f'model.layers.{i}.self_attn.q_proj')
        target_modules.append(f'model.layers.{i}.self_attn.k_proj')
        target_modules.append(f'model.layers.{i}.self_attn.v_proj')
        target_modules.append(f'model.layers.{i}.self_attn.o_proj')
        target_modules.append(f'model.layers.{i}.mlp.gate_proj')
        target_modules.append(f'model.layers.{i}.mlp.up_proj')
        target_modules.append(f'model.layers.{i}.mlp.down_proj')
    target_modules += ['mm_projector.0', 'mm_projector.2']

    peft_config = LoraConfig(
        r=128,
        lora_alpha=128,
        target_modules=target_modules,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        # modules_to_save=["mm_projector"],
    )
    model = get_peft_model(model, peft_config)

    return model

def main(fabric):
    import torch
    # torch.set_default_dtype(torch.bfloat16)
    from torch.utils.data import Dataset, DataLoader

    from transformers import AutoProcessor, AutoModelForCausalLM
    from transformers import LlavaNextForConditionalGeneration
    from transformers import BitsAndBytesConfig
    from trl import DataCollatorForCompletionOnlyLM
    from bitsandbytes.optim import AdamW8bit
    from lightning.pytorch import utilities as pt_utils

    tokenizer, image_processor, model_config = load_processor()
    train_dataset = load_dataset(tokenizer, image_processor, model_config)

    # testing
    d = train_dataset[0]

    instruction_template = "<|start_header_id|>user<|end_header_id|>\n\n"
    response_template = "<|start_header_id|>assistant<|end_header_id|>\n\n"
    collator = DataCollatorForCompletionOnlyLM(
        instruction_template=instruction_template,
        response_template=response_template,
        tokenizer=tokenizer,
        mlm=False,
    )

    model = load_model()
    if RESUME:
        model = load_lora_model(model)
    else:
        model = get_lora_model(model)


    model.print_trainable_parameters()
    model.gradient_checkpointing_enable(
        gradient_checkpointing_kwargs={"use_reentrant": False},
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        collate_fn=collator,
        num_workers=4,
        shuffle=True,
        drop_last=True,
        pin_memory=False,
    )

    LR = 1e-4
    total_steps = len(train_loader) // (GRAD_ACC_STEPS * fabric.world_size) * NUM_EPOCH
    if fabric.global_rank == 0:
        print(f"total steps: {total_steps}")

    resume_epoch = 0
    global_step = 0
    start_iteration = 0

    if RESUME:

        optimizer = AdamW8bit(
            [p for p in model.parameters() if p.requires_grad],
            lr=LR,
            weight_decay=0.001,
        )
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=LR,
            pct_start=0.01,
            total_steps=total_steps,
            anneal_strategy="linear",
        )

        model, optimizer = fabric.setup(model, optimizer)

        checkpoint = torch.load(f'{RESUME_PATH}/opt_ckp.pth')
        optimizer.param_groups[0] = checkpoint['optimizer']
        start_iteration = checkpoint['iteration']
        resume_epoch = checkpoint['epoch']
        global_step = checkpoint['global_step']
        scheduler = checkpoint['scheduler']

    else:
        optimizer = AdamW8bit(
            [p for p in model.parameters() if p.requires_grad],
            lr=LR,
            weight_decay=0.001,
        )
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=LR,
            pct_start=0.01,
            total_steps=total_steps,
            anneal_strategy="linear",
        )

        model, optimizer = fabric.setup(model, optimizer)

    dataloader = fabric.setup_dataloaders(train_loader)

    model.train()
    for epoch in range(resume_epoch, NUM_EPOCH):
        batch_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch}", disable=fabric.global_rank != 0)
        for iteration, batch in enumerate(pbar, start=start_iteration):
            is_accumulating = iteration % GRAD_ACC_STEPS != 0
            loss = model(**batch, use_cache=False).loss
            fabric.backward(loss)
            batch_loss += loss.item()
            if not is_accumulating:
                optimizer.step()
                optimizer.zero_grad()
                scheduler.step()
                fabric.log_dict({
                    "trainer/loss": batch_loss/GRAD_ACC_STEPS,
                    "trainer/lr": optimizer.param_groups[0]["lr"],
                    "trainer/epoch": epoch,
                    "trainer/global_step": global_step,
                })

                if fabric.global_rank == 0:
                    pbar.set_postfix({
                        "loss": batch_loss/GRAD_ACC_STEPS,
                        "lr": optimizer.param_groups[0]["lr"],
                        "global_step": global_step,
                    })
                    tqdm.write(f"loss: {batch_loss/GRAD_ACC_STEPS:.4f}, lr: {optimizer.param_groups[0]['lr']:.6f}, global_step: {global_step}")

                batch_loss = 0
                global_step += 1

            if (global_step+1) % 200 == 0 and fabric.global_rank == 0:
                save_dir = f"./ckp"
                os.makedirs(save_dir, exist_ok=True)
                save_path = f"{save_dir}/checkpoint-{global_step}"

                model.save_pretrained(save_path)
                tokenizer.save_pretrained(save_path)

                print(f"Succesfully saved model at {save_path}")

            if (global_step+1) % 2 == 0 and fabric.global_rank == 0 and not is_accumulating:
                save_dir = f"./ckp"
                os.makedirs(save_dir, exist_ok=True)

                # save checkpoint
                save_path = f"{save_dir}/checkpoint-latest"
                model.save_pretrained(save_path)
                tokenizer.save_pretrained(save_path)

                # save optimizer and schedulser
                opt_checkpoint = {
                    'epoch': epoch,
                    'iteration':iteration,
                    'global_step': global_step,
                    'optimizer': optimizer.param_groups[0],
                    'scheduler': scheduler}

                torch.save(opt_checkpoint, f'{save_path}/opt_ckp.pth')
                print(f"Succesfully saved model at {save_path}")


            torch.cuda.empty_cache()

# %%
import torch
from lightning.fabric.strategies import DeepSpeedStrategy

# %%
logger = WandbLogger(project="sam4mllm")

fabric = Fabric(
    accelerator="cuda",
    devices=10,
    precision="bf16-true",
    strategy=DeepSpeedStrategy(
        zero_optimization=True,
        stage=2,
    ),
    loggers=logger,
)

fabric.launch(main)


#################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/SAM4MLLM/data/dataset_cls.py
# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.


import random
from PIL import Image

import torch
from torch.utils.data import Dataset

from llava.mm_utils import process_images, tokenizer_image_token
from llava.constants import IMAGE_TOKEN_INDEX

SYSTEM_PROMPT = "You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language."

class RefDataset(Dataset):
    def __init__(self, data, tokenizer, img_processor, model_config, n_points=10,
                 img_dir='/home/ai2lab/work/datasets/', system_prompts=None):
        self.data = data
        self.tokenizer = tokenizer
        self.img_processor = img_processor
        self.model_config = model_config
        self.n_points = n_points
        self.img_dir = img_dir

        self.system_prompts = [SYSTEM_PROMPT] if system_prompts is None else system_prompts

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        example_d = self.data[idx]
        image_path = example_d['image_path']
        image = Image.open(self.img_dir + image_path).resize((672, 672))

        conv = [{"role": "system", "content": random.choice(self.system_prompts)}]

        s_phrase = random.choice(example_d['phrases'])
        answer_counts = example_d['answer_counts']
        question_box = f'<image>\nPlease provide the bounding box coordinate of the region this sentence describes ({answer_counts}):\n"{s_phrase}".'
        if len(example_d['bboxes']) == 0:
            answer_box = 'No object found.'
        else:
            answer_box = ' '.join([f'[{x[0]},{x[1]},{x[2]},{x[3]}]' for x in example_d['bboxes']])

        conv.extend([
            {"role": "user", "content": question_box},
            {"role": "assistant", "content": answer_box}
        ])

        for bbox, p_n_ls in zip(example_d['bboxes'], example_d['points_and_labels']):
            n_sel_points = random.randint(10, 30)
            sampled_points_and_labels = random.sample(p_n_ls, n_sel_points)

            points_txt = ' '.join([f'[{x[0]},{x[1]}]' for x in sampled_points_and_labels])
            question_points = 'Check if the points listed below are located on the object with coordinates [{},{},{},{}]:\n{}'.format(
                bbox[0], bbox[1], bbox[2], bbox[3], points_txt)
            answer_points = ''.join(['Yes' if x[2] else 'No' for x in sampled_points_and_labels])

            conv.extend([
                {"role": "user", "content": question_points},
                {"role": "assistant", "content": answer_points}
            ])

        conv_text = self.tokenizer.apply_chat_template(conv, tokenize=False)
        input_ids = tokenizer_image_token(conv_text, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt")

        pixel_values = process_images([image], self.img_processor, self.model_config)[0].half()
        image_sizes = [672, 672]

        sample = {
            'input_ids': input_ids,
            'images': pixel_values,
            'image_sizes': image_sizes
        }

        return sample


class GroupRefDataset(Dataset):
    def __init__(
            self,
            img_grouped_data,
            tokenizer,
            img_processor,
            model_config,
            n_points=10,
            img_dir='/home/ai2lab/work/datasets/',
            max_len=4096,
            img_size=(672,672),
            system_prompts=None
        ):
        self.img_grouped_data = img_grouped_data
        self.tokenizer = tokenizer
        self.img_processor = img_processor
        self.model_config = model_config
        self.n_points = n_points
        self.img_dir = img_dir
        self.max_len = max_len
        self.img_size = img_size

        if system_prompts is None:
            self.system_prompts = [SYSTEM_PROMPT]

        self.ref_data = []
        self.reload(0)

    def __len__(self):
        return len(self.ref_data)

    def reload(self, seed):
        self.ref_data = []
        n_ref_per_img = 6

        print(f'Dataset reloaded with seed {seed}')
        random.seed(seed)
        for img_path, refs in self.img_grouped_data.items():
            random.shuffle(refs)
            for i in range(0, len(refs), n_ref_per_img):
                self.ref_data.append(refs[i:i+n_ref_per_img])

    def __getitem__(self, idx, tokenize=True):
        ref_samples = self.ref_data[idx]
        image_path = ref_samples[0]['image_path']
        image = Image.open(self.img_dir + image_path).resize(self.img_size)

        conv = [{"role": "system", "content": random.choice(self.system_prompts)}]

        for i_conv, ref_sample in enumerate(ref_samples):

            s_phrase = random.choice(ref_sample['phrases'])
            answer_counts = ref_sample['answer_counts']
            question_box = '<image>\n' if i_conv == 0 else ''
            question_box += f'Please provide the bounding box coordinate of the region this sentence describes ({answer_counts}):\n"{s_phrase}".'
            if len(ref_sample['bboxes']) == 0:
                answer_box = 'No object found.'
            else:
                answer_box = ' '.join([f'[{x[0]},{x[1]},{x[2]},{x[3]}]' for x in ref_sample['bboxes']])

            conv.extend([
                {"role": "user", "content": question_box},
                {"role": "assistant", "content": answer_box}
            ])

            for bbox, p_n_ls in zip(ref_sample['bboxes'], ref_sample['points_and_labels']):
                bbox_area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1]) / 1000 / 1000
                n_sel_points = random.randint(10, 30)
                sampled_points_and_labels = random.sample(p_n_ls, n_sel_points)

                points_txt = ' '.join([f'[{x[0]},{x[1]}]' for x in sampled_points_and_labels])
                question_points = 'Check if the points listed below are located on the object with coordinates [{},{},{},{}]:\n{}'.format(
                    bbox[0], bbox[1], bbox[2], bbox[3], points_txt)
                answer_points = ''.join(['Yes' if x[2] else 'No' for x in sampled_points_and_labels])

                conv.extend([
                    {"role": "user", "content": question_points},
                    {"role": "assistant", "content": answer_points}
                ])

        conv_text = self.tokenizer.apply_chat_template(conv, tokenize=False)

        if tokenize:
            input_ids = tokenizer_image_token(conv_text, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt")
            input_ids = input_ids[:self.max_len]
        else:
            input_ids = conv_text

        pixel_values = process_images([image], self.img_processor, self.model_config)[0].half()
        image_sizes = list(self.img_size)

        sample = {
            'input_ids': input_ids,
            'images': pixel_values,
            'image_sizes': image_sizes
        }

        return sample


class GroupRefDatasetHf(Dataset):
    def __init__(
            self,
            img_grouped_data,
            processor,
            img_str='<image>',
            n_point_range=(10, 30),
            img_dir='/home/ai2lab/work/datasets/',
            max_len=4096,
            img_size=(672,672),
            system_prompts=None,
            n_ref_per_img=5,
        ):
        self.img_grouped_data = img_grouped_data
        self.processor = processor
        self.img_str = img_str
        self.n_point_range = n_point_range
        self.img_dir = img_dir
        self.max_len = max_len
        self.img_size = img_size
        self.n_ref_per_img = n_ref_per_img
        self.system_prompts = system_prompts

        self.ref_data = []
        self.reload()

    def __len__(self):
        return len(self.ref_data)

    def reload(self):
        self.ref_data = []

        for img_path, refs in self.img_grouped_data.items():
            random.shuffle(refs)
            for i in range(0, len(refs), self.n_ref_per_img):
                self.ref_data.append(refs[i:i+self.n_ref_per_img])

    def __getitem__(self, idx, tokenize=True):
        ref_samples = self.ref_data[idx]
        image_path = ref_samples[0]['image_path']
        image = Image.open(self.img_dir + image_path).resize(self.img_size)

        conv = []
        if self.system_prompts is not None:
            conv.extend[{"role": "system", "content": random.choice(self.system_prompts)}]

        for i_conv, ref_sample in enumerate(ref_samples):

            s_phrase = random.choice(ref_sample['phrases'])
            answer_counts = ref_sample['answer_counts']
            question_box = f'{self.img_str}\n' if i_conv == 0 else ''
            question_box += f'Please provide the bounding box coordinate of the region this sentence describes ({answer_counts}):\n"{s_phrase}".'

            if len(ref_sample['bboxes']) == 0:
                answer_box = 'No object found.'
            else:
                answer_box = ' '.join([f'[{x[0]:03d},{x[1]:03d},{x[2]:03d},{x[3]:03d}]' for x in ref_sample['bboxes']])
            answer_box = ' ' + answer_box

            question_box = question_box + '\n' if question_box[-1] != '\n' else question_box
            # answer_box = answer_box + '\n' if answer_box[-1] != '\n' else answer_box
            conv.extend([
                {"role": "user", "content": question_box},
                {"role": "assistant", "content": answer_box}
            ])

            for bbox, p_n_ls in zip(ref_sample['bboxes'], ref_sample['points_and_labels']):
                n_sel_points = random.randint(self.n_point_range[0], self.n_point_range[1])
                sampled_points_and_labels = random.sample(p_n_ls, n_sel_points)

                points_txt = ' '.join([f'[{x[0]:03d},{x[1]:03d}]' for x in sampled_points_and_labels])
                question_points = 'Check if the points listed below are located on the object with coordinates [{:03d},{:03d},{:03d},{:03d}]:\n{}'.format(
                    bbox[0], bbox[1], bbox[2], bbox[3], points_txt)
                answer_points = '_' + ''.join(['Yes' if x[2] else 'No' for x in sampled_points_and_labels])

                # question_points = question_points + '\n' if question_points[-1] != '\n' else question_points
                # answer_points = answer_points + '\n' if answer_points[-1] != '\n' else answer_points
                conv.extend([
                    {"role": "user", "content": question_points},
                    {"role": "assistant", "content": answer_points}
                ])

        conv_text = self.processor.tokenizer.apply_chat_template(conv, tokenize=False)
        if conv_text.startswith('<s>'):
            conv_text = conv_text[3:]

        if tokenize:
            encoded = self.processor(conv_text, [image], return_tensors="pt")
            encoded['input_ids'] = encoded['input_ids'][0][:self.max_len]
            encoded['attention_mask'] = encoded['attention_mask'][0][:self.max_len]
            encoded['pixel_values'] = encoded['pixel_values'][0]
            encoded['image_sizes'] = encoded['image_sizes'][0]
        else:
            encoded = {
                'input_ids': conv_text,
                'images': [image],
                'image_sizes': [[image.size[0], image.size[1]]]
            }

        return encoded


class GroupRefDatasetHfV2(Dataset):
    def __init__(
            self,
            img_grouped_data,
            processor,
            img_str='<image>',
            n_point_range=(10, 30),
            img_dir='/home/ai2lab/work/datasets/',
            max_len=4096,
            img_size=(672,672),
            system_prompts=None,
            n_ref_per_img=5,
        ):
        self.img_grouped_data = img_grouped_data
        self.processor = processor
        self.img_str = img_str
        self.n_point_range = n_point_range
        self.img_dir = img_dir
        self.max_len = max_len
        self.img_size = img_size
        self.n_ref_per_img = n_ref_per_img
        self.system_prompts = system_prompts

        self.ref_data = []
        self.reload()

    def __len__(self):
        return len(self.ref_data)

    def reload(self):
        self.ref_data = []

        for img_path, refs in self.img_grouped_data.items():
            random.shuffle(refs)
            for i in range(0, len(refs), self.n_ref_per_img):
                self.ref_data.append(refs[i:i+self.n_ref_per_img])

    def __getitem__(self, idx, tokenize=True):
        ref_samples = self.ref_data[idx]
        image_path = ref_samples[0]['image_path']
        image = Image.open(self.img_dir + image_path).resize(self.img_size)

        conv = []
        if self.system_prompts is not None:
            conv.extend([{"role": "system", "content": random.choice(self.system_prompts)}])

        random.shuffle(ref_samples)
        is_over_max_len = False
        total_bbox = sum([len(x['bboxes']) for x in ref_samples])
        for i_conv, ref_sample in enumerate(ref_samples):

            if is_over_max_len:
                break

            s_phrase = random.choice(ref_sample['phrases'])
            answer_counts = ref_sample['answer_counts']
            question_box = f'{self.img_str}\n' if i_conv == 0 else ''
            question_box += f'Please provide the bounding box coordinate of the region this sentence describes ({answer_counts}):\n"{s_phrase}".'

            if len(ref_sample['bboxes']) == 0:
                answer_box = 'No object found.'
            else:
                answer_box = ' '.join([f'[{x[0]:03d},{x[1]:03d},{x[2]:03d},{x[3]:03d}]' for x in ref_sample['bboxes']])

            conv.extend([
                {"role": "user", "content": question_box},
                {"role": "assistant", "content": answer_box}
            ])
            conv_tokens = self.processor.tokenizer.apply_chat_template(conv, tokenize=True)
            if len(conv_tokens) > self.max_len:
                print('cut off')
                conv = conv[:-2]
                is_over_max_len = True
                break

            for bbox, p_n_ls in zip(ref_sample['bboxes'], ref_sample['points_and_labels']):
                n_sel_points = random.randint(self.n_point_range[0], self.n_point_range[1])
                if total_bbox <= 3:
                    n_sel_points = self.n_point_range[1]

                sampled_points_and_labels = random.sample(p_n_ls, n_sel_points)

                points_txt = ' '.join([f'[{x[0]:03d},{x[1]:03d}]' for x in sampled_points_and_labels])
                question_points = 'Check if the points listed below are located on the object with coordinates [{:03d},{:03d},{:03d},{:03d}]:\n{}'.format(
                    bbox[0], bbox[1], bbox[2], bbox[3], points_txt)
                answer_points = ''.join(['Yes' if x[2] else 'No' for x in sampled_points_and_labels])

                conv.extend([
                    {"role": "user", "content": question_points},
                    {"role": "assistant", "content": answer_points}
                ])
                conv_tokens = self.processor.tokenizer.apply_chat_template(conv, tokenize=True)
                if len(conv_tokens) > self.max_len:
                    print('cut off')
                    conv = conv[:-2]
                    is_over_max_len = True
                    break

        conv_text = self.processor.tokenizer.apply_chat_template(conv, tokenize=False)
        if conv_text.startswith('<s>'):
            conv_text = conv_text[3:]
        if conv_text.startswith('<|begin_of_text|>'):
            conv_text = conv_text[17:]

        if tokenize:
            encoded = self.processor(conv_text, [image], return_tensors="pt")
            encoded['input_ids'] = encoded['input_ids'][0][:self.max_len]
            encoded['attention_mask'] = encoded['attention_mask'][0][:self.max_len]
            encoded['pixel_values'] = encoded['pixel_values'][0].to(torch.bfloat16)
            encoded['image_sizes'] = encoded['image_sizes'][0]
        else:
            encoded = {
                'input_ids': conv_text,
                'images': [image],
                'image_sizes': [[image.size[0], image.size[1]]]
            }

        return encoded



#################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/SAM4MLLM/data/__init__.py
# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.


from .dataset_cls import RefDataset, GroupRefDataset, GroupRefDatasetHf, GroupRefDatasetHfV2

#################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/SAM4MLLM/data/utils.py
# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.


import random
import numpy as np
from scipy.stats import qmc

from pycocotools import mask as maskUtils

# convert coco segmentation to binary mask
def seg_to_mask(seg, img_h, img_w):
    if isinstance(seg, list):
        rles = maskUtils.frPyObjects(seg, img_h, img_w)
        rle = maskUtils.merge(rles)
    elif isinstance(seg['counts'], list):
        rle = maskUtils.frPyObjects(seg, img_h, img_w)
    else:
        rle = seg
    return maskUtils.decode(rle)

def sample_points_from_bbox(bbox, num_samples=5):
    x1, y1, x2, y2 = bbox
    points = [(random.uniform(x1, x2), random.uniform(y1, y2)) for _ in range(num_samples)]
    return points

class PoissonDiskSampler:
    def __init__(self, num_samples=100, radius=0.1, num_cache=100):
        self.num_samples = num_samples
        self.radius = radius

        self.cache_pattern = []
        for _ in range(num_cache):
            rng = np.random.default_rng()
            engine = qmc.PoissonDisk(d=2, radius=radius, seed=rng)
            sample = engine.random(num_samples)
            self.cache_pattern.append(sample)

    def sample(self, bbox, use_cache=True):
        x1, y1, x2, y2 = bbox
        if use_cache:
            sample = random.choice(self.cache_pattern).copy()
        else:
            rng = np.random.default_rng()
            engine = qmc.PoissonDisk(d=2, radius=self.radius, seed=rng)
            sample = engine.random(self.num_samples)

        sample[:, 0] = sample[:, 0] * (x2 - x1) + x1
        sample[:, 1] = sample[:, 1] * (y2 - y1) + y1
        return sample

class DotDict(dict):
    def __getattr__(self, name):
        return self[name]

    def __setattr__(self, name, value):
        self[name] = value

    def __delattr__(self, name):
        del self[name]

#################################################################



<my code for changing>
# /home/dbcloud/PycharmProjects/mllm4sam/app/main.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/main.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto. Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

import argparse
import yaml
import torch
from torch.utils.data import DataLoader

from dataloader.dataset_sam4mllm import BaseSAM4MLLMDataset
from models.model import SAM4MLLMModel
from engine.trainer import Trainer
from util.utils import set_seed


###############################################################################
# Example main script
###############################################################################
def main(args):
    # -------------------------------------------------------------------------
    # 1. Load Config
    # -------------------------------------------------------------------------
    with open(args.config, "r") as f:
        config = yaml.safe_load(f)

    # fix seed
    set_seed(config.get("seed", 42))

    # Retrieve from config
    sam_model_path = config["model"]["sam"]
    qwen_model_path = config["model"]["qwen"]
    print(f"[DEBUG] SAM model path: {sam_model_path}")
    print(f"[DEBUG] Qwen model path: {qwen_model_path}")

    # -------------------------------------------------------------------------
    # 2. Decide how to load data
    # -------------------------------------------------------------------------
    # If the user sets `use_data: "woundsegmentation"`, we let the dataset
    # load from CSV automatically by passing None for data_list.
    # If the user sets `use_data: "dummy"`, we show a small example list.
    #
    # This ensures we do NOT hit the FileNotFoundError from "dummy1.png"
    # unless you intentionally choose "dummy".
    #
    if config["dataset"]["use_data"] == "woundsegmentation":
        train_data_list = None
        val_data_list = None
        print("[INFO] Using the woundsegmentation CSV data from root_dir.")
    else:
        # fallback dummy
        print("[INFO] Using dummy data_list since use_data != woundsegmentation.")
        train_data_list = [
            # Example placeholders. In real usage, you might rely on `use_data`
            # to auto-load from disk with segmentation maps, etc.
            {"image_path": "dummy1.png", "mask_path": "dummy1_mask.png", "conversation": "Segment the object."},
            {"image_path": "dummy2.png", "mask_path": "dummy2_mask.png", "conversation": "Segment the big region."}
        ]
        val_data_list = [
            {"image_path": "dummy3.png", "mask_path": "dummy3_mask.png", "conversation": "Find me the object."}
        ]

    # -------------------------------------------------------------------------
    # 3. Prepare Dataset / Dataloader
    # -------------------------------------------------------------------------
    train_dataset = BaseSAM4MLLMDataset(
        data_list=train_data_list,   # can be None if we want CSV-based loading
        tokenizer=None,             # The actual Qwen tokenizer is loaded inside the backbone
        transforms=None,
        max_len=config["train"]["max_len"],
        img_size=tuple(config["train"]["img_size"]),
        img_dir=config["train"]["img_dir"],
        system_prompt="You are a helpful segmentation assistant.",
        use_data=config["dataset"]["use_data"],
        root_dir=config["dataset"]["root_dir"],
        split=config["dataset"]["split"]
    )

    # For validation dataset, we typically do "split='test'" if there's a separate CSV/test set.
    # But we rely on config["dataset"]["split"] = "train" or "test" as you prefer.
    # For demonstration, we keep it the same. You might point it to a test CSV if you have one.
    val_dataset = BaseSAM4MLLMDataset(
        data_list=val_data_list,     # can be None if we want CSV-based loading for val
        tokenizer=None,
        transforms=None,
        max_len=config["train"]["max_len"],
        img_size=tuple(config["train"]["img_size"]),
        img_dir=config["train"]["img_dir"],
        system_prompt="You are a helpful segmentation assistant.",
        use_data=config["dataset"]["use_data"],
        root_dir=config["dataset"]["root_dir"],
        split=config["dataset"]["split"]  # or "test" if you want separate test set
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=config["train"]["batch_size"],
        shuffle=True,
        num_workers=0
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config["train"]["batch_size"],
        shuffle=False,
        num_workers=0
    )

    # -------------------------------------------------------------------------
    # 4. Build Model
    # -------------------------------------------------------------------------
    from models.blocks.qwen_sam_backbone import QwenSamBackbone

    # This backbone will internally load Qwen + SAM (but SAM is not trained).
    backbone = QwenSamBackbone(
        qwen_model_path=qwen_model_path,
        sam_model_path=sam_model_path,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    model = SAM4MLLMModel(backbone=backbone)

    # -------------------------------------------------------------------------
    # 5. Trainer
    # -------------------------------------------------------------------------
    trainer = Trainer(
        model=model,
        train_dataloader=train_loader,
        val_dataloader=val_loader,
        lr=config["optimizer"]["lr"],
        max_epochs=config["train"]["epochs"],
        grad_acc_steps=config["train"]["grad_acc_steps"],
        scheduler_type=config["optimizer"]["scheduler_type"],
        warmup_steps=config["optimizer"]["warmup_steps"],
        early_stop_patience=config["train"]["early_stop_patience"],
        clip_grad_norm=config["train"]["clip_grad_norm"],
        output_dir=config["train"]["output_dir"],
        run_name=config["train"]["run_name"],
        device="cuda" if torch.cuda.is_available() else "cpu",
        use_amp=config["train"]["use_amp"],
        log_interval=config["train"]["log_interval"]
    )

    # -------------------------------------------------------------------------
    # 6. Train
    # -------------------------------------------------------------------------
    trainer.train()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="config.yaml", help="Path to config")
    args = parser.parse_args()
    main(args)


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/config.yaml
seed: 42

dataset:
  use_data: "woundsegmentation"  # or "dummy"
  root_dir: "/home/dbcloud//wound_segmentation_classification_processed"
  split: "train"

train:
  max_len: 1536
  img_size: [512, 512]
  img_dir: "./data_images"
  batch_size: 2
  epochs: 3
  grad_acc_steps: 1
  early_stop_patience: 3
  clip_grad_norm: 1.0
  output_dir: "runs"
  run_name: "sam4mllm_demo"
  use_amp: true
  log_interval: 2

optimizer:
  lr: 1e-4
  scheduler_type: "linear"  # or "cosine"
  warmup_steps: 1000

model:
  sam: "/home/dbcloud/PycharmProjects/mllm4sam/sam-vit-base"          # Path or HF repo for your SAM checkpoint if needed
  qwen: "/home/dbcloud/PycharmProjects/mllm4sam/Qwen2-VL-2B-Instruct"    # Path or HF repo for Qwen model


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/models/model.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/models/model.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.

import torch
import torch.nn as nn

###############################################################################
# SAM4MLLM Model
###############################################################################
class SAM4MLLMModel(nn.Module):
    """
    This wrapper now holds a QwenSamBackbone, which uses Qwen-VL and optionally
    SAM synergy. We do NOT train SAM, only Qwen (with LoRA).
    """

    def __init__(self, backbone, projector=None):
        super().__init__()
        self.backbone = backbone
        self.projector = projector  # e.g. if you want extra layers

    def forward(self, input_ids, attention_mask, images=None, **kwargs):
        outputs = self.backbone(
            input_ids=input_ids,
            attention_mask=attention_mask,
            images=images,
            **kwargs
        )
        return outputs

    def generate(self, input_ids, attention_mask, images=None, max_new_tokens=128, **kwargs):
        if hasattr(self.backbone, "generate"):
            return self.backbone.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                images=images,
                max_new_tokens=max_new_tokens,
                **kwargs
            )
        else:
            print(f'[INFO] Model does not support generation.')
            return None


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/models/blocks/visual_encoder.py
# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto. Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

import torch
import torch.nn as nn

class VisualEncoder(nn.Module):
    """
    A placeholder for a custom visual encoder that might process images
    before feeding into your main backbone. For instance,
    you can incorporate SAM ViT or any other custom logic here.
    """
    def __init__(self, embed_dim=768):
        super().__init__()
        # In practice, you might load a pretrained SAM encoder or similar
        self.conv = nn.Conv2d(3, embed_dim, kernel_size=7, stride=2, padding=3)

    def forward(self, x: torch.Tensor):
        """
        x shape: (B, 3, H, W)
        """
        # Dummy forward
        feats = self.conv(x)
        return feats


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/models/blocks/qwen_sam_backbone.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/models/blocks/qwen_sam_backbone.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto. Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.
#
# -------------------------------------------------------------------------
# This code provides a synergy backbone that integrates Qwen2-VL and (optionally)
# a SAM model from Hugging Face Transformers. It addresses shape mismatch issues
# by dynamically overriding Qwen2-VL's default vision config. If your input images
# are 512x512, you must set patch_size to a divisor of 512 (e.g., 16 or 32).
# You must also set temporal_patch_size=1 if you're treating each image as a
# single frame. Otherwise, the Qwen2-VL code (which defaults to patch_size=14
# and temporal_patch_size=2) will raise a shape mismatch error.
#
# Important notes:
#  1) If you feed 512x512 images, an integer patch size that divides 512
#     is needed. For example, patch_size=16 => 512/16=32 patches per dimension.
#  2) For single-frame images, set temporal_patch_size=1. This effectively
#     avoids 3D patch slicing across time.
#  3) This code prints debug shapes for clarity and possible further debugging.
#
# Example usage:
#   from models.blocks.qwen_sam_backbone import QwenSamBackbone
#   backbone = QwenSamBackbone(
#       qwen_model_path="path_or_repo_to_Qwen2-VL",
#       sam_model_path="path_or_repo_to_SAM",
#       device="cuda",
#       override_patch_size=16,         # adjust if you want a different patch size
#       override_temporal_patch_size=1  # must be 1 if you have single images
#   )
#   model = YourTrainerWrapper(backbone=backbone)
#   ...
#
# -------------------------------------------------------------------------

import os
import sys
import torch
import torch.nn as nn

from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from transformers.models.qwen2_vl.modeling_qwen2_vl import PatchEmbed

# Attempt to import Hugging Face SamModel, SamProcessor:
try:
    from transformers import SamModel, SamProcessor
    HF_SAM_AVAILABLE = True
except ImportError as e:
    print("[WARNING] Could not import SamModel, SamProcessor from transformers. Reason:", e)
    HF_SAM_AVAILABLE = False

# Attempt to import PEFT for LoRA:
try:
    from peft import LoraConfig, get_peft_model
    LOADING_PEFT_OK = True
except Exception as e:
    print("[WARNING] Could not import peft or bitsandbytes. Reason:", e)
    LOADING_PEFT_OK = False

# Attempt to import Qwen2VLConfig and Qwen2VLVisionConfig to override patch sizes:
try:
    from transformers.models.qwen2_vl import Qwen2VLConfig, Qwen2VLVisionConfig
    QWEN_CONFIG_AVAILABLE = True
except ImportError as e:
    print("[WARNING] Could not import Qwen2VLConfig or Qwen2VLVisionConfig. Reason:", e)
    QWEN_CONFIG_AVAILABLE = False


class QwenSamBackbone(nn.Module):
    """
    A synergy backbone that loads Qwen2-VL with Qwen2VLForConditionalGeneration
    (and optionally HF's SamModel) for demonstration.

    Key improvement:
      - We handle image patch size incompatibility by directly modifying the patch embedding layer
      - We calculate and provide the proper grid_thw parameter for rotary embeddings
      - If environment or GPU does not allow LoRA, it will be disabled gracefully.
      - Debug prints for shape mismatch resolution.
    """

    def __init__(
            self,
            qwen_model_path: str,
            sam_model_path: str,
            device="cuda",
            override_patch_size: int = 16,  # Changed from 14 to 16 to be divisible with 512x512
            override_temporal_patch_size: int = 1,  # Changed from 2 to 1 for single frame images
    ):
        """
        Args:
            qwen_model_path (str):
                Path or HF repo ID for Qwen2-VL. Must be a Qwen2-VL 2.0 or 2.1 style checkpoint.
            sam_model_path (str):
                Path or HF repo ID for SAM (optional).
            device (str):
                "cuda" or "cpu", etc.
            override_patch_size (int):
                The patch size for height/width dimension in Qwen2-VL. If your image is 512x512,
                a patch_size that divides 512 exactly is recommended (e.g., 16 or 32).
            override_temporal_patch_size (int):
                The patch size for temporal dimension. If you're not dealing with video frames,
                set 1. The default Qwen code sets 2, which often causes shape mismatch for single images.
        """
        super().__init__()
        self.device = device
        self.override_patch_size = override_patch_size
        self.override_temporal_patch_size = override_temporal_patch_size

        # Load the model without trying to modify the config first
        print(f"[INFO] Loading Qwen2-VL model from {qwen_model_path}...")
        self.qwen_model = Qwen2VLForConditionalGeneration.from_pretrained(
            qwen_model_path,
            torch_dtype=torch.float16,
            trust_remote_code=True
        ).to(device)

        # Load the Qwen processor
        print("[INFO] Loading Qwen2-VL processor (tokenizer + image processor) ...")
        self.qwen_processor = AutoProcessor.from_pretrained(
            qwen_model_path,
            trust_remote_code=True
        )
        self.qwen_tokenizer = self.qwen_processor.tokenizer
        if self.qwen_tokenizer.pad_token is None:
            self.qwen_tokenizer.pad_token = self.qwen_tokenizer.eos_token

        # Now we modify the vision model's patch embedding layer directly
        print(f"[INFO] Overriding patch embeddings with patch_size={override_patch_size}, "
              f"temporal_patch_size={override_temporal_patch_size}")

        # Create a new patch embedding layer with our desired parameters
        orig_embed = self.qwen_model.visual.patch_embed
        new_patch_embed = PatchEmbed(
            patch_size=override_patch_size,
            temporal_patch_size=override_temporal_patch_size,
            in_channels=orig_embed.in_channels,
            embed_dim=orig_embed.embed_dim
        ).to(device).to(self.qwen_model.dtype)

        # Replace the patch embedding layer in the model
        self.qwen_model.visual.patch_embed = new_patch_embed

        # Manually set the rope scaling to match our new patch size
        if hasattr(self.qwen_model.config, "rope_scaling") and self.qwen_model.config.rope_scaling is not None:
            # For mrope_section, use the same value as original patch_size or divide by 4
            if "mrope_section" not in self.qwen_model.config.rope_scaling:
                self.qwen_model.config.rope_scaling["mrope_section"] = [
                    override_patch_size // 4,
                    override_patch_size // 4 * 3 // 2,  # Scaled to match original ratio
                    override_patch_size // 4 * 3 // 2  # Scaled to match original ratio
                ]
            print(f"[INFO] Using mrope_section = {self.qwen_model.config.rope_scaling['mrope_section']}")

        # Attempt LoRA setup
        if LOADING_PEFT_OK:
            try:
                print("[INFO] Attempting to set up LoRA for Qwen2-VL ...")
                self._setup_lora()
            except Exception as e:
                print("[WARNING] LoRA setup failed due to environment error. Disabling LoRA. Error:", e)
        else:
            print("[WARNING] LoRA is disabled because we cannot import PEFT properly.")

        # Attempt to load HF SAM
        self.sam_model = None
        self.sam_processor = None
        if HF_SAM_AVAILABLE:
            try:
                print(f"[INFO] Attempting to load SAM model from {sam_model_path} via HF Transformers SamModel...")
                self.sam_model = SamModel.from_pretrained(
                    sam_model_path,
                    torch_dtype=torch.float16
                ).eval().to(device)
                self.sam_processor = SamProcessor.from_pretrained(sam_model_path)
            except Exception as e:
                print(f"[WARNING] Could not load huggingface SamModel from {sam_model_path}. Reason: {e}")
                self.sam_model = None
                self.sam_processor = None
        else:
            print("[WARNING] SamModel is not available from transformers package. Skipping HF-SAM usage.")

    def _setup_lora(self):
        """
        Create and apply a LoRA adapter config to Qwen2-VL if environment permits.
        If it fails, we skip LoRA.
        """
        # You should refine which modules to target. As a minimal example, we do:
        target_modules = [
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj",
        ]
        # We skip the lm_head and norms. Tweak as needed.
        lora_config = LoraConfig(
            r=16,
            lora_alpha=16,
            target_modules=target_modules,
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        self.qwen_model = get_peft_model(self.qwen_model, lora_config)
        self.qwen_model.print_trainable_parameters()

    def _calculate_grid_thw(self, images):
        """
        Calculate the grid_thw parameter for Qwen2-VL based on the image shape and patch sizes.

        Args:
            images (torch.Tensor): Tensor of shape [batch_size, channels, height, width]

        Returns:
            torch.Tensor: Tensor of shape [batch_size, 3] containing (time, height, width) grid dimensions
        """
        if images is None:
            return None

        batch_size = images.shape[0]
        _, _, height, width = images.shape

        # Calculate number of patches in each dimension
        # For single images (not video), temporal dimension is always 1
        t = 1  # We're dealing with single images, not videos
        h = height // self.override_patch_size
        w = width // self.override_patch_size

        # Create grid_thw tensor: [batch_size, 3]
        grid_thw = torch.tensor([[t, h, w] for _ in range(batch_size)],
                                device=images.device,
                                dtype=torch.long)

        print(f"[DEBUG] Calculated grid_thw shape: {grid_thw.shape}, values: {grid_thw[0]}")
        return grid_thw

    def forward(self, input_ids, attention_mask, images=None, labels=None, **kwargs):
        """
        Standard forward pass for Qwen2VLForConditionalGeneration:
          - 'images' => pixel_values for multi-modal Qwen2-VL
          - If labels are provided, we get cross-entropy loss from Qwen2-VL
          - If no labels, we get raw logits
        """
        # Debug shapes
        print(f"[DEBUG QwenSamBackbone] forward() input_ids shape: {input_ids.shape}")
        print(f"[DEBUG QwenSamBackbone] forward() attention_mask shape: {attention_mask.shape}")
        if images is not None:
            print(f"[DEBUG QwenSamBackbone] forward() images shape: {images.shape}")
            # Important: print the full tensor shape to help diagnose
            # potential reshaping issues
            image_size = images.size()
            num_elements = images.numel()
            print(f"[DEBUG QwenSamBackbone] images size: {image_size}, num_elements: {num_elements}")

            # Calculate the grid_thw parameter based on image dimensions and patch size
            image_grid_thw = self._calculate_grid_thw(images)
        else:
            image_grid_thw = None

        # The Qwen2-VL code expects 'pixel_values' for images
        if images is not None:
            outputs = self.qwen_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=images,
                image_grid_thw=image_grid_thw,  # Pass the calculated grid_thw
                labels=labels,
                **kwargs
            )
        else:
            outputs = self.qwen_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                **kwargs
            )
        return outputs

    def generate(self, input_ids, attention_mask, images=None, max_new_tokens=128, **kwargs):
        """
        For text generation (multi-modal or text-only).
        """
        print(f"[DEBUG QwenSamBackbone] generate() input_ids shape: {input_ids.shape}")
        print(f"[DEBUG QwenSamBackbone] generate() attention_mask shape: {attention_mask.shape}")
        if images is not None:
            print(f"[DEBUG QwenSamBackbone] generate() images shape: {images.shape}")

            # Calculate the grid_thw parameter for generation based on image dimensions and patch size
            image_grid_thw = self._calculate_grid_thw(images)
        else:
            image_grid_thw = None

        if images is not None:
            outputs = self.qwen_model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=images,
                image_grid_thw=image_grid_thw,  # Pass the calculated grid_thw
                max_new_tokens=max_new_tokens,
                **kwargs
            )
        else:
            outputs = self.qwen_model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                **kwargs
            )
        return outputs

###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/engine/trainer.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/engine/trainer.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.

import os
import time
import math
import csv
import wandb
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
from datetime import datetime
from torch.cuda.amp import autocast, GradScaler

from util.logger import log_validation_samples
from util.utils import set_seed, ensure_dir
from engine.evaluator import Evaluator

class Trainer:
    def __init__(self,
                 model,
                 train_dataloader,
                 val_dataloader=None,
                 lr=1e-4,
                 max_epochs=3,
                 grad_acc_steps=1,
                 scheduler_type="linear",
                 warmup_steps=1000,
                 early_stop_patience=5,
                 clip_grad_norm=1.0,
                 output_dir="runs",
                 run_name="sam4mllm",
                 device="cuda",
                 use_amp=True,
                 log_interval=10):
        self.model = model
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader

        self.lr = lr
        self.max_epochs = max_epochs
        self.grad_acc_steps = grad_acc_steps
        self.scheduler_type = scheduler_type
        self.warmup_steps = warmup_steps
        self.early_stop_patience = early_stop_patience
        self.clip_grad_norm = clip_grad_norm

        self.output_dir = output_dir
        dt_string = datetime.now().strftime("%Y%m%d-%H%M%S")
        self.run_dir = os.path.join(self.output_dir, f"{run_name}_{dt_string}")
        ensure_dir(self.run_dir)

        self.device = device
        self.use_amp = use_amp
        self.log_interval = log_interval

        self.global_step = 0
        self.best_val_loss = float("inf")
        self.epochs_no_improve = 0
        self.should_stop = False

        # Move model to device
        self.model.to(self.device)

        # Setup wandb
        wandb.init(project="SAM4MLLM", name=run_name, dir=self.run_dir)

        # Setup logging files
        self.train_log_path = os.path.join(self.run_dir, "train_log.csv")
        self.val_log_path = os.path.join(self.run_dir, "val_log.csv")

        with open(self.train_log_path, "w", newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["step", "epoch", "loss", "lr"])
        with open(self.val_log_path, "w", newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["step", "epoch", "val_loss"])

        # Setup optimizer
        # Only LoRA parameters are "trainable" in the Qwen model
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        self.optimizer = optim.AdamW(trainable_params, lr=float(self.lr))

        # Setup scheduler
        num_training_steps = len(self.train_dataloader) // self.grad_acc_steps * self.max_epochs
        self.lr_scheduler = self._create_scheduler(self.optimizer, num_training_steps)

        # Setup grad scaler
        self.scaler = GradScaler(enabled=self.use_amp)

        # Evaluator
        if self.val_dataloader is not None:
            self.evaluator = Evaluator(self.model, self.device)
        else:
            self.evaluator = None

        # Folder for saving sample predictions
        self.val_samples_dir = os.path.join(self.run_dir, "val_samples")
        ensure_dir(self.val_samples_dir)

    def _create_scheduler(self, optimizer, num_training_steps):
        if self.scheduler_type == "linear":
            def lr_lambda(current_step: int):
                if current_step < self.warmup_steps:
                    return float(current_step) / float(max(1, self.warmup_steps))
                return max(
                    0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - self.warmup_steps))
                )
            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        elif self.scheduler_type == "cosine":
            def lr_lambda(current_step: int):
                if current_step < self.warmup_steps:
                    return float(current_step) / float(max(1, self.warmup_steps))
                progress = float(current_step - self.warmup_steps) / float(max(1, num_training_steps - self.warmup_steps))
                return 0.5 * (1.0 + math.cos(math.pi * progress))
            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        else:
            raise ValueError(f"Unknown scheduler: {self.scheduler_type}")
        return scheduler

    def train(self):
        for epoch in range(self.max_epochs):
            print(f"\n=== [Epoch {epoch+1}/{self.max_epochs}] ===")
            self.model.train()
            epoch_loss = 0.0
            step_count = 0

            for step, batch in enumerate(tqdm(self.train_dataloader, desc="Training")):
                # Move to device
                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)
                images = batch.get("image", None)
                if images is not None and isinstance(images, torch.Tensor):
                    images = images.to(self.device)

                loss_val = self._training_step(input_ids, attention_mask, images)
                epoch_loss += loss_val
                step_count += 1

                if (step + 1) % self.log_interval == 0:
                    avg_loss = epoch_loss / step_count
                    lr_now = self.optimizer.param_groups[0]["lr"]
                    print(f"Epoch [{epoch+1}] Step [{step+1}/{len(self.train_dataloader)}], "
                          f"Loss: {avg_loss:.4f}, LR: {lr_now:.6f}")
                    wandb.log({"train/loss": avg_loss, "train/lr": lr_now, "step": self.global_step})
                    with open(self.train_log_path, "a", newline='') as f:
                        writer = csv.writer(f)
                        writer.writerow([self.global_step, epoch+1, avg_loss, lr_now])

            # End of epoch
            if self.val_dataloader is not None:
                val_loss = self.validate(epoch+1)
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.epochs_no_improve = 0
                    self._save_checkpoint(is_best=True, epoch=epoch+1)
                else:
                    self.epochs_no_improve += 1
                    if self.epochs_no_improve >= self.early_stop_patience:
                        print("Early stopping triggered!")
                        self.should_stop = True
                        break
            else:
                self._save_checkpoint(is_best=False, epoch=epoch+1)

            if self.should_stop:
                break

        # Save final
        self._save_checkpoint(is_best=False, epoch=self.max_epochs, last=True)

    def _training_step(self, input_ids, attention_mask, images):
        # We let the huggingface trainer do cross-entropy if we pass `labels`.
        # So we'll pass `labels=input_ids` in a typical "teacher forcing" next token style
        # but it requires we shift the tokens ourselves or rely on the built-in logic.
        with autocast(enabled=self.use_amp):
            # The HF CausalLM automatically uses next-token if we pass labels=input_ids
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                images=images,
                labels=input_ids
            )
            loss = outputs.loss if hasattr(outputs, "loss") else outputs[0]

        loss_for_backward = loss / self.grad_acc_steps
        self.scaler.scale(loss_for_backward).backward()

        if (self.global_step + 1) % self.grad_acc_steps == 0:
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad_norm)
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.optimizer.zero_grad()
            self.lr_scheduler.step()

        self.global_step += 1
        return loss.item()

    def validate(self, epoch):
        self.model.eval()
        val_losses = []
        # We'll store the samples for later logging
        sample_storage = []
        with torch.no_grad():
            for i, batch in enumerate(tqdm(self.val_dataloader, desc=f"Validation Epoch {epoch}")):
                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)
                images = batch.get("image", None)
                if images is not None and isinstance(images, torch.Tensor):
                    images = images.to(self.device)

                with autocast(enabled=self.use_amp):
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        images=images,
                        labels=input_ids
                    )
                    loss = outputs.loss if hasattr(outputs, "loss") else outputs[0]
                    val_losses.append(loss.item())

                # Store the first sample in this batch for logging
                if i < 3:  # we log up to 3
                    sample_storage.append({
                        "input_ids": input_ids[0].detach().cpu(),
                        "attention_mask": attention_mask[0].detach().cpu(),
                        "points_str": batch.get("points_str", None)
                    })

        val_loss = sum(val_losses) / len(val_losses) if len(val_losses) > 0 else 0.0
        with open(self.val_log_path, "a", newline='') as f:
            writer = csv.writer(f)
            writer.writerow([self.global_step, epoch, val_loss])

        print(f"[Validation] Epoch: {epoch}, Loss: {val_loss:.4f}")
        wandb.log({"val/loss": val_loss, "epoch": epoch, "step": self.global_step})

        # Now log the 3 samples
        log_validation_samples(
            model=self.model,
            val_samples=sample_storage,
            device=self.device,
            global_step=self.global_step,
            out_dir=self.val_samples_dir,
            tokenizer=getattr(self.model.backbone, "qwen_tokenizer", None)
        )
        return val_loss

    def _save_checkpoint(self, is_best=False, epoch=0, last=False):
        name = "best_model.pt" if is_best else "last_model.pt" if last else f"checkpoint_epoch_{epoch}.pt"
        save_path = os.path.join(self.run_dir, name)
        torch.save({
            "epoch": epoch,
            "global_step": self.global_step,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.lr_scheduler.state_dict(),
            "scaler_state_dict": self.scaler.state_dict()
        }, save_path)
        print(f"Checkpoint saved to {save_path}")


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/engine/evaluator.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/engine/evaluator.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.

import torch
import torch.nn as nn

class Evaluator:
    """
    Minimal evaluator skeleton for SAM4MLLM.
    We do not do advanced metrics here. We let the trainer handle
    the cross-entropy loss. This class can be expanded if needed.
    """
    def __init__(self, model: nn.Module, device="cuda"):
        self.model = model
        self.device = device

    def evaluate(self, dataloader):
        self.model.eval()
        losses = []
        with torch.no_grad():
            for batch in dataloader:
                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)
                images = batch.get("image", None)
                if images is not None and isinstance(images, torch.Tensor):
                    images = images.to(self.device)

                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    images=images,
                    labels=input_ids
                )
                loss = outputs.loss if hasattr(outputs, "loss") else outputs[0]
                losses.append(loss.item())

        mean_loss = sum(losses) / len(losses) if len(losses) > 0 else 0
        return {"loss": mean_loss}


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/util/logger.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/util/logger.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.

import wandb
import torch

def parse_points_from_text(text):
    """
    Attempt to parse lines like '(12,34), (56,78)' from the predicted text
    Return a set of (x, y) tuples.
    """
    # naive parse
    import re
    pattern = r"\(\s*(\d+)\s*,\s*(\d+)\s*\)"
    matches = re.findall(pattern, text)
    points = set()
    for match in matches:
        x = int(match[0])
        y = int(match[1])
        points.add((x, y))
    return points

def compute_point_match(pred_set, gt_str):
    """
    Count how many points in pred_set also appear in the ground truth.
    The ground truth is a string like '(x1,y1), (x2,y2)...' or "NoValidPoints".
    """
    if not gt_str or gt_str == "NoValidPoints":
        return 1.0 if (len(pred_set) == 0) else 0.0

    gt_points = parse_points_from_text(gt_str)
    if len(gt_points) == 0:
        # No GT
        if len(pred_set) == 0:
            return 1.0
        else:
            return 0.0

    # measure fraction of GT that was predicted
    correct = len(pred_set.intersection(gt_points))
    total = len(gt_points)
    return float(correct) / float(total) if total > 0 else 0.0

def log_validation_samples(model, val_samples, device, global_step, out_dir, tokenizer=None):
    """
    For each sample in val_samples, decode the model's predictions and compute
    a naive "point match" with the ground truth string.
    """
    if not val_samples:
        return

    model.eval()
    for idx, sample in enumerate(val_samples):
        input_ids = sample["input_ids"].unsqueeze(0).to(device)
        attention_mask = sample["attention_mask"].unsqueeze(0).to(device)
        gt_str = sample.get("points_str", None)
        if gt_str is not None and isinstance(gt_str, list):
            gt_str = gt_str[0]  # if it was batched

        with torch.no_grad():
            gen_tokens = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=50
            )
        if tokenizer is not None:
            pred_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)
        else:
            # fallback
            pred_text = str(gen_tokens[0].tolist())

        # parse points
        pred_points = parse_points_from_text(pred_text)
        match_score = compute_point_match(pred_points, gt_str)

        # Log to wandb
        log_dict = {
            f"val_sample_{idx}/pred_text": pred_text,
            f"val_sample_{idx}/ground_truth_points": gt_str,
            f"val_sample_{idx}/match_score": match_score,
            "global_step": global_step
        }
        wandb.log(log_dict)


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/util/metrics.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/util/metrics.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.

def compute_iou(pred_mask, gt_mask):
    """
    Example IoU computation placeholder.
    pred_mask, gt_mask: (H, W) boolean or 0/1 arrays
    """
    intersection = (pred_mask & gt_mask).sum()
    union = (pred_mask | gt_mask).sum()
    if union == 0:
        return 1.0
    return float(intersection) / float(union)


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/util/utils.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/util/utils.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.

import os
import random
import numpy as np
import torch

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def ensure_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/dataloader/dataset_sam4mllm.py
import os
import csv
import random
import torch
import numpy as np
import PIL
from torch.utils.data import Dataset

def mask_to_points(mask_array, max_points=10):
    """
    Convert a binary segmentation mask to a list of (x,y) pixel coordinates
    lying inside the mask. Return up to max_points of them.
    """
    coords = np.argwhere(mask_array == 1)  # shape (N, 2)
    if len(coords) == 0:
        return []
    coords_list = coords.tolist()
    random.shuffle(coords_list)
    coords_list = coords_list[:max_points]
    return coords_list

class BaseSAM4MLLMDataset(Dataset):
    """
    A base dataset for SAM4MLLM demonstration that also
    extracts random points from the segmentation mask as "ground truth."
    We will present these points in the "assistant" portion to do
    next-token cross-entropy training on Qwen's text output.

    This version addresses:
     - Forcing the image to 224 x 224 to match Qwen2-VL shape expectations.
     - Additional debug prints for shape mismatch issues.
     - Optional CSV loading for 'woundsegmentation' scenario.
     - Using system_prompt to unify conversation text.

    If the user sets `use_data: "woundsegmentation"`, then we attempt to load
    a CSV with image/mask pairs from root_dir. Otherwise, we fallback to a
    user-provided data_list or a dummy example.
    """

    def __init__(self,
                 data_list=None,
                 tokenizer=None,
                 transforms=None,
                 max_len=1536,
                 img_size=(224, 224),   # We now default to 224x224
                 img_dir='./data_images/',
                 system_prompt="You are a helpful segmentation assistant.",
                 use_data="dummy",
                 root_dir="",
                 split="train"):
        super().__init__()
        self.tokenizer = tokenizer
        self.transforms = transforms
        self.max_len = max_len
        # We force 224 x 224 by default for Qwen2-VL
        self.img_size = img_size
        self.img_dir = img_dir
        self.system_prompt = system_prompt
        self.use_data = use_data
        self.root_dir = root_dir
        self.split = split

        # Load or set data_list
        if data_list is not None:
            self.data_list = data_list
        else:
            self.data_list = []
            if self.use_data == "woundsegmentation":
                self.data_list = self._load_woundsegmentation_data()
            else:
                # fallback dummy
                self.data_list = [
                    {"image_path": "dummy1.png", "mask_path": "dummy1_mask.png", "conversation": "Segment?"},
                    {"image_path": "dummy2.png", "mask_path": "dummy2_mask.png", "conversation": "Segment?"}
                ]

        self._prune_missing_files()

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx: int):
        item = self.data_list[idx]
        image_path = item.get("image_path", None)
        mask_path = item.get("mask_path", None)
        conversation = item.get("conversation", "")

        if not image_path:
            raise ValueError(f"No 'image_path' found for index {idx} in data_list.")

        # 1. Load image
        full_image_path = self._resolve_path(image_path, subfolder="images")
        if not os.path.exists(full_image_path):
            raise FileNotFoundError(f"Image file not found even after pruning: {full_image_path}")
        print(f"[DEBUG Dataset] Loading image: {full_image_path}")

        image = PIL.Image.open(full_image_path).convert("RGB")

        # 2. Load mask if mask_path is available
        mask = None
        if mask_path:
            full_mask_path = self._resolve_path(mask_path, subfolder="masks")
            if os.path.exists(full_mask_path):
                print(f"[DEBUG Dataset] Loading mask: {full_mask_path}")
                mask = PIL.Image.open(full_mask_path).convert("L")
            else:
                print(f"[DEBUG Dataset] No mask found at {full_mask_path}, skipping mask.")
                mask = None

        # 3. transforms or resizing to 224x224
        #    Qwen2-VLs patch-embedding requires (B, 3, 224, 224) by default
        if self.transforms:
            image = self.transforms(image)
            if mask is not None:
                mask = self.transforms(mask)
        else:
            image = image.resize(self.img_size)
            if mask is not None:
                mask = mask.resize(self.img_size)

        # 4. Convert mask to up to 10 points
        max_points = 10
        points_str = ""
        if mask is not None:
            mask_np = np.array(mask, dtype=np.uint8)
            mask_np = (mask_np >= 128).astype(np.uint8)
            coords_list = mask_to_points(mask_np, max_points=max_points)
            if len(coords_list) > 0:
                points_str_list = []
                for (y, x) in coords_list:
                    points_str_list.append(f"({x},{y})")
                points_str = ", ".join(points_str_list)
            else:
                points_str = "NoValidPoints"
        else:
            # If no mask, we have no valid points
            points_str = "NoValidPoints"

        # 5. Build textual prompt
        user_text = f"Please provide up to 10 points that cover the object region."
        assistant_text = points_str
        full_text = (
            f"{self.system_prompt}\n[USER]: {conversation}\n"
            f"[USER]: {user_text}\n"
            f"[ASSISTANT]: {assistant_text}"
        )

        # 6. Tokenize if available
        if self.tokenizer is not None:
            tokens = self.tokenizer(
                full_text,
                return_tensors="pt",
                truncation=True,
                max_length=self.max_len
            )
            input_ids = tokens["input_ids"].squeeze(0)
            attention_mask = tokens["attention_mask"].squeeze(0)
        else:
            # placeholders
            input_ids = torch.tensor([0])
            attention_mask = torch.tensor([1])

        # 7. Convert image to tensor if still PIL
        if isinstance(image, PIL.Image.Image):
            image = torch.from_numpy(np.array(image)).permute(2, 0, 1)
        # Debug shape
        print(f"[DEBUG Dataset] Final image shape: {image.shape}")

        sample = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "image": image,
            "points_str": points_str
        }
        return sample

    def _resolve_path(self, filename, subfolder="images"):
        """
        1) Fix common issues if 'train/images/' or 'test/images/' are already in 'filename'
           so we don't double them.
        2) Attempt to see if it's an absolute path. If so, return it.
        3) Otherwise, check if it's in self.img_dir
        4) Else, fallback to root_dir/split/subfolder/filename
        """
        fix_list = [
            "train/images/", "test/images/",
            "train/masks/", "test/masks/"
        ]
        for fix_token in fix_list:
            if fix_token in filename:
                # print(f"[DEBUG Dataset] Stripping '{fix_token}' from filename: {filename}")
                filename = filename.replace(fix_token, "")

        # Now proceed
        if os.path.isabs(filename):
            return filename

        potential_path = os.path.join(self.img_dir, filename)
        if os.path.exists(potential_path):
            return potential_path

        alt = os.path.join(self.root_dir, self.split, subfolder, filename)
        return alt

    def _load_woundsegmentation_data(self):
        """
        If `use_data == "woundsegmentation"`, we read from:
          root_dir/train_labels.csv or root_dir/test_labels.csv
        to build data_list with image_path, mask_path, conversation, etc.
        """
        data_list = []
        csv_name = "train_labels.csv" if self.split == "train" else "test_labels.csv"
        csv_path = os.path.join(self.root_dir, csv_name)
        if not os.path.exists(csv_path):
            print(f"[WARNING] CSV file not found at {csv_path}. Returning empty data_list.")
            return data_list

        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            for row in reader:
                if not row:
                    continue
                filename = row[0].strip()
                data_item = {
                    "image_path": filename,
                    "mask_path": filename,
                    "conversation": "Help me segment this wound."
                }
                data_list.append(data_item)
        return data_list

    def _prune_missing_files(self):
        kept = []
        for item in self.data_list:
            resolved = self._resolve_path(item["image_path"], subfolder="images")
            if not os.path.exists(resolved):
                print(f"[WARNING] Skipping nonexistent file: {resolved}")
                continue
            kept.append(item)
        self.data_list = kept
        print(f"[INFO] After pruning, we have {len(self.data_list)} valid samples.")


###################################################################


# /home/dbcloud/PycharmProjects/mllm4sam/app/main.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/main.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto. Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

import argparse
import yaml
import torch
from torch.utils.data import DataLoader

from dataloader.dataset_sam4mllm import BaseSAM4MLLMDataset
from models.model import SAM4MLLMModel
from engine.trainer import Trainer
from util.utils import set_seed


###############################################################################
# Example main script
###############################################################################
def main(args):
    # -------------------------------------------------------------------------
    # 1. Load Config
    # -------------------------------------------------------------------------
    with open(args.config, "r") as f:
        config = yaml.safe_load(f)

    # fix seed
    set_seed(config.get("seed", 42))

    # Retrieve from config
    sam_model_path = config["model"]["sam"]
    qwen_model_path = config["model"]["qwen"]
    print(f"[DEBUG] SAM model path: {sam_model_path}")
    print(f"[DEBUG] Qwen model path: {qwen_model_path}")

    # -------------------------------------------------------------------------
    # 2. Decide how to load data
    # -------------------------------------------------------------------------
    # If the user sets `use_data: "woundsegmentation"`, we let the dataset
    # load from CSV automatically by passing None for data_list.
    # If the user sets `use_data: "dummy"`, we show a small example list.
    #
    # This ensures we do NOT hit the FileNotFoundError from "dummy1.png"
    # unless you intentionally choose "dummy".
    #
    if config["dataset"]["use_data"] == "woundsegmentation":
        train_data_list = None
        val_data_list = None
        print("[INFO] Using the woundsegmentation CSV data from root_dir.")
    else:
        # fallback dummy
        print("[INFO] Using dummy data_list since use_data != woundsegmentation.")
        train_data_list = [
            # Example placeholders. In real usage, you might rely on `use_data`
            # to auto-load from disk with segmentation maps, etc.
            {"image_path": "dummy1.png", "mask_path": "dummy1_mask.png", "conversation": "Segment the object."},
            {"image_path": "dummy2.png", "mask_path": "dummy2_mask.png", "conversation": "Segment the big region."}
        ]
        val_data_list = [
            {"image_path": "dummy3.png", "mask_path": "dummy3_mask.png", "conversation": "Find me the object."}
        ]

    # -------------------------------------------------------------------------
    # 3. Prepare Dataset / Dataloader
    # -------------------------------------------------------------------------
    train_dataset = BaseSAM4MLLMDataset(
        data_list=train_data_list,   # can be None if we want CSV-based loading
        tokenizer=None,             # The actual Qwen tokenizer is loaded inside the backbone
        transforms=None,
        max_len=config["train"]["max_len"],
        img_size=tuple(config["train"]["img_size"]),
        img_dir=config["train"]["img_dir"],
        system_prompt="You are a helpful segmentation assistant.",
        use_data=config["dataset"]["use_data"],
        root_dir=config["dataset"]["root_dir"],
        split=config["dataset"]["split"]
    )

    # For validation dataset, we typically do "split='test'" if there's a separate CSV/test set.
    # But we rely on config["dataset"]["split"] = "train" or "test" as you prefer.
    # For demonstration, we keep it the same. You might point it to a test CSV if you have one.
    val_dataset = BaseSAM4MLLMDataset(
        data_list=val_data_list,     # can be None if we want CSV-based loading for val
        tokenizer=None,
        transforms=None,
        max_len=config["train"]["max_len"],
        img_size=tuple(config["train"]["img_size"]),
        img_dir=config["train"]["img_dir"],
        system_prompt="You are a helpful segmentation assistant.",
        use_data=config["dataset"]["use_data"],
        root_dir=config["dataset"]["root_dir"],
        split=config["dataset"]["split"]  # or "test" if you want separate test set
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=config["train"]["batch_size"],
        shuffle=True,
        num_workers=0
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config["train"]["batch_size"],
        shuffle=False,
        num_workers=0
    )

    # -------------------------------------------------------------------------
    # 4. Build Model
    # -------------------------------------------------------------------------
    from models.blocks.qwen_sam_backbone import QwenSamBackbone

    # This backbone will internally load Qwen + SAM (but SAM is not trained).
    backbone = QwenSamBackbone(
        qwen_model_path=qwen_model_path,
        sam_model_path=sam_model_path,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    model = SAM4MLLMModel(backbone=backbone)

    # -------------------------------------------------------------------------
    # 5. Trainer
    # -------------------------------------------------------------------------
    trainer = Trainer(
        model=model,
        train_dataloader=train_loader,
        val_dataloader=val_loader,
        lr=config["optimizer"]["lr"],
        max_epochs=config["train"]["epochs"],
        grad_acc_steps=config["train"]["grad_acc_steps"],
        scheduler_type=config["optimizer"]["scheduler_type"],
        warmup_steps=config["optimizer"]["warmup_steps"],
        early_stop_patience=config["train"]["early_stop_patience"],
        clip_grad_norm=config["train"]["clip_grad_norm"],
        output_dir=config["train"]["output_dir"],
        run_name=config["train"]["run_name"],
        device="cuda" if torch.cuda.is_available() else "cpu",
        use_amp=config["train"]["use_amp"],
        log_interval=config["train"]["log_interval"]
    )

    # -------------------------------------------------------------------------
    # 6. Train
    # -------------------------------------------------------------------------
    trainer.train()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="config.yaml", help="Path to config")
    args = parser.parse_args()
    main(args)


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/config.yaml
seed: 42

dataset:
  use_data: "woundsegmentation"  # or "dummy"
  root_dir: "/home/dbcloud//wound_segmentation_classification_processed"
  split: "train"

train:
  max_len: 1536
  img_size: [512, 512]
  img_dir: "./data_images"
  batch_size: 2
  epochs: 3
  grad_acc_steps: 1
  early_stop_patience: 3
  clip_grad_norm: 1.0
  output_dir: "runs"
  run_name: "sam4mllm_demo"
  use_amp: true
  log_interval: 2

optimizer:
  lr: 1e-4
  scheduler_type: "linear"  # or "cosine"
  warmup_steps: 1000

model:
  sam: "/home/dbcloud/PycharmProjects/mllm4sam/sam-vit-base"          # Path or HF repo for your SAM checkpoint if needed
  qwen: "/home/dbcloud/PycharmProjects/mllm4sam/Qwen2-VL-2B-Instruct"    # Path or HF repo for Qwen model


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/models/model.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/models/model.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.

import torch
import torch.nn as nn

###############################################################################
# SAM4MLLM Model
###############################################################################
class SAM4MLLMModel(nn.Module):
    """
    This wrapper now holds a QwenSamBackbone, which uses Qwen-VL and optionally
    SAM synergy. We do NOT train SAM, only Qwen (with LoRA).
    """

    def __init__(self, backbone, projector=None):
        super().__init__()
        self.backbone = backbone
        self.projector = projector  # e.g. if you want extra layers

    def forward(self, input_ids, attention_mask, images=None, **kwargs):
        outputs = self.backbone(
            input_ids=input_ids,
            attention_mask=attention_mask,
            images=images,
            **kwargs
        )
        return outputs

    def generate(self, input_ids, attention_mask, images=None, max_new_tokens=128, **kwargs):
        if hasattr(self.backbone, "generate"):
            return self.backbone.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                images=images,
                max_new_tokens=max_new_tokens,
                **kwargs
            )
        else:
            print(f'[INFO] Model does not support generation.')
            return None


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/models/blocks/visual_encoder.py
# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto. Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

import torch
import torch.nn as nn

class VisualEncoder(nn.Module):
    """
    A placeholder for a custom visual encoder that might process images
    before feeding into your main backbone. For instance,
    you can incorporate SAM ViT or any other custom logic here.
    """
    def __init__(self, embed_dim=768):
        super().__init__()
        # In practice, you might load a pretrained SAM encoder or similar
        self.conv = nn.Conv2d(3, embed_dim, kernel_size=7, stride=2, padding=3)

    def forward(self, x: torch.Tensor):
        """
        x shape: (B, 3, H, W)
        """
        # Dummy forward
        feats = self.conv(x)
        return feats


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/models/blocks/qwen_sam_backbone.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/models/blocks/qwen_sam_backbone.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto. Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.
#
# -------------------------------------------------------------------------
# This code provides a synergy backbone that integrates Qwen2-VL and (optionally)
# a SAM model from Hugging Face Transformers. It addresses shape mismatch issues
# by dynamically overriding Qwen2-VL's default vision config. If your input images
# are 512x512, you must set patch_size to a divisor of 512 (e.g., 16 or 32).
# You must also set temporal_patch_size=1 if you're treating each image as a
# single frame. Otherwise, the Qwen2-VL code (which defaults to patch_size=14
# and temporal_patch_size=2) will raise a shape mismatch error.
#
# Important notes:
#  1) If you feed 512x512 images, an integer patch size that divides 512
#     is needed. For example, patch_size=16 => 512/16=32 patches per dimension.
#  2) For single-frame images, set temporal_patch_size=1. This effectively
#     avoids 3D patch slicing across time.
#  3) This code prints debug shapes for clarity and possible further debugging.
#
# Example usage:
#   from models.blocks.qwen_sam_backbone import QwenSamBackbone
#   backbone = QwenSamBackbone(
#       qwen_model_path="path_or_repo_to_Qwen2-VL",
#       sam_model_path="path_or_repo_to_SAM",
#       device="cuda",
#       override_patch_size=16,         # adjust if you want a different patch size
#       override_temporal_patch_size=1  # must be 1 if you have single images
#   )
#   model = YourTrainerWrapper(backbone=backbone)
#   ...
#
# -------------------------------------------------------------------------

import os
import sys
import torch
import torch.nn as nn

from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from transformers.models.qwen2_vl.modeling_qwen2_vl import PatchEmbed

# Attempt to import Hugging Face SamModel, SamProcessor:
try:
    from transformers import SamModel, SamProcessor
    HF_SAM_AVAILABLE = True
except ImportError as e:
    print("[WARNING] Could not import SamModel, SamProcessor from transformers. Reason:", e)
    HF_SAM_AVAILABLE = False

# Attempt to import PEFT for LoRA:
try:
    from peft import LoraConfig, get_peft_model
    LOADING_PEFT_OK = True
except Exception as e:
    print("[WARNING] Could not import peft or bitsandbytes. Reason:", e)
    LOADING_PEFT_OK = False

# Attempt to import Qwen2VLConfig and Qwen2VLVisionConfig to override patch sizes:
try:
    from transformers.models.qwen2_vl import Qwen2VLConfig, Qwen2VLVisionConfig
    QWEN_CONFIG_AVAILABLE = True
except ImportError as e:
    print("[WARNING] Could not import Qwen2VLConfig or Qwen2VLVisionConfig. Reason:", e)
    QWEN_CONFIG_AVAILABLE = False


class QwenSamBackbone(nn.Module):
    """
    A synergy backbone that loads Qwen2-VL with Qwen2VLForConditionalGeneration
    (and optionally HF's SamModel) for demonstration.

    Key improvement:
      - We handle image patch size incompatibility by directly modifying the patch embedding layer
      - We calculate and provide the proper grid_thw parameter for rotary embeddings
      - If environment or GPU does not allow LoRA, it will be disabled gracefully.
      - Debug prints for shape mismatch resolution.
    """

    def __init__(
            self,
            qwen_model_path: str,
            sam_model_path: str,
            device="cuda",
            override_patch_size: int = 16,  # Changed from 14 to 16 to be divisible with 512x512
            override_temporal_patch_size: int = 1,  # Changed from 2 to 1 for single frame images
    ):
        """
        Args:
            qwen_model_path (str):
                Path or HF repo ID for Qwen2-VL. Must be a Qwen2-VL 2.0 or 2.1 style checkpoint.
            sam_model_path (str):
                Path or HF repo ID for SAM (optional).
            device (str):
                "cuda" or "cpu", etc.
            override_patch_size (int):
                The patch size for height/width dimension in Qwen2-VL. If your image is 512x512,
                a patch_size that divides 512 exactly is recommended (e.g., 16 or 32).
            override_temporal_patch_size (int):
                The patch size for temporal dimension. If you're not dealing with video frames,
                set 1. The default Qwen code sets 2, which often causes shape mismatch for single images.
        """
        super().__init__()
        self.device = device
        self.override_patch_size = override_patch_size
        self.override_temporal_patch_size = override_temporal_patch_size

        # Load the model without trying to modify the config first
        print(f"[INFO] Loading Qwen2-VL model from {qwen_model_path}...")
        self.qwen_model = Qwen2VLForConditionalGeneration.from_pretrained(
            qwen_model_path,
            torch_dtype=torch.float16,
            trust_remote_code=True
        ).to(device)

        # Load the Qwen processor
        print("[INFO] Loading Qwen2-VL processor (tokenizer + image processor) ...")
        self.qwen_processor = AutoProcessor.from_pretrained(
            qwen_model_path,
            trust_remote_code=True
        )
        self.qwen_tokenizer = self.qwen_processor.tokenizer
        if self.qwen_tokenizer.pad_token is None:
            self.qwen_tokenizer.pad_token = self.qwen_tokenizer.eos_token

        # Now we modify the vision model's patch embedding layer directly
        print(f"[INFO] Overriding patch embeddings with patch_size={override_patch_size}, "
              f"temporal_patch_size={override_temporal_patch_size}")

        # Create a new patch embedding layer with our desired parameters
        orig_embed = self.qwen_model.visual.patch_embed
        new_patch_embed = PatchEmbed(
            patch_size=override_patch_size,
            temporal_patch_size=override_temporal_patch_size,
            in_channels=orig_embed.in_channels,
            embed_dim=orig_embed.embed_dim
        ).to(device).to(self.qwen_model.dtype)

        # Replace the patch embedding layer in the model
        self.qwen_model.visual.patch_embed = new_patch_embed

        # Manually set the rope scaling to match our new patch size
        if hasattr(self.qwen_model.config, "rope_scaling") and self.qwen_model.config.rope_scaling is not None:
            # For mrope_section, use the same value as original patch_size or divide by 4
            if "mrope_section" not in self.qwen_model.config.rope_scaling:
                self.qwen_model.config.rope_scaling["mrope_section"] = [
                    override_patch_size // 4,
                    override_patch_size // 4 * 3 // 2,  # Scaled to match original ratio
                    override_patch_size // 4 * 3 // 2  # Scaled to match original ratio
                ]
            print(f"[INFO] Using mrope_section = {self.qwen_model.config.rope_scaling['mrope_section']}")

        # Attempt LoRA setup
        if LOADING_PEFT_OK:
            try:
                print("[INFO] Attempting to set up LoRA for Qwen2-VL ...")
                self._setup_lora()
            except Exception as e:
                print("[WARNING] LoRA setup failed due to environment error. Disabling LoRA. Error:", e)
        else:
            print("[WARNING] LoRA is disabled because we cannot import PEFT properly.")

        # Attempt to load HF SAM
        self.sam_model = None
        self.sam_processor = None
        if HF_SAM_AVAILABLE:
            try:
                print(f"[INFO] Attempting to load SAM model from {sam_model_path} via HF Transformers SamModel...")
                self.sam_model = SamModel.from_pretrained(
                    sam_model_path,
                    torch_dtype=torch.float16
                ).eval().to(device)
                self.sam_processor = SamProcessor.from_pretrained(sam_model_path)
            except Exception as e:
                print(f"[WARNING] Could not load huggingface SamModel from {sam_model_path}. Reason: {e}")
                self.sam_model = None
                self.sam_processor = None
        else:
            print("[WARNING] SamModel is not available from transformers package. Skipping HF-SAM usage.")

    def _setup_lora(self):
        """
        Create and apply a LoRA adapter config to Qwen2-VL if environment permits.
        If it fails, we skip LoRA.
        """
        # You should refine which modules to target. As a minimal example, we do:
        target_modules = [
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj",
        ]
        # We skip the lm_head and norms. Tweak as needed.
        lora_config = LoraConfig(
            r=16,
            lora_alpha=16,
            target_modules=target_modules,
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        self.qwen_model = get_peft_model(self.qwen_model, lora_config)
        self.qwen_model.print_trainable_parameters()

    def _calculate_grid_thw(self, images):
        """
        Calculate the grid_thw parameter for Qwen2-VL based on the image shape and patch sizes.

        Args:
            images (torch.Tensor): Tensor of shape [batch_size, channels, height, width]

        Returns:
            torch.Tensor: Tensor of shape [batch_size, 3] containing (time, height, width) grid dimensions
        """
        if images is None:
            return None

        batch_size = images.shape[0]
        _, _, height, width = images.shape

        # Calculate number of patches in each dimension
        # For single images (not video), temporal dimension is always 1
        t = 1  # We're dealing with single images, not videos
        h = height // self.override_patch_size
        w = width // self.override_patch_size

        # Create grid_thw tensor: [batch_size, 3]
        grid_thw = torch.tensor([[t, h, w] for _ in range(batch_size)],
                                device=images.device,
                                dtype=torch.long)

        print(f"[DEBUG] Calculated grid_thw shape: {grid_thw.shape}, values: {grid_thw[0]}")
        return grid_thw

    def forward(self, input_ids, attention_mask, images=None, labels=None, **kwargs):
        """
        Standard forward pass for Qwen2VLForConditionalGeneration:
          - 'images' => pixel_values for multi-modal Qwen2-VL
          - If labels are provided, we get cross-entropy loss from Qwen2-VL
          - If no labels, we get raw logits
        """
        # Debug shapes
        print(f"[DEBUG QwenSamBackbone] forward() input_ids shape: {input_ids.shape}")
        print(f"[DEBUG QwenSamBackbone] forward() attention_mask shape: {attention_mask.shape}")
        if images is not None:
            print(f"[DEBUG QwenSamBackbone] forward() images shape: {images.shape}")
            # Important: print the full tensor shape to help diagnose
            # potential reshaping issues
            image_size = images.size()
            num_elements = images.numel()
            print(f"[DEBUG QwenSamBackbone] images size: {image_size}, num_elements: {num_elements}")

            # Calculate the grid_thw parameter based on image dimensions and patch size
            image_grid_thw = self._calculate_grid_thw(images)
        else:
            image_grid_thw = None

        # The Qwen2-VL code expects 'pixel_values' for images
        if images is not None:
            outputs = self.qwen_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=images,
                image_grid_thw=image_grid_thw,  # Pass the calculated grid_thw
                labels=labels,
                **kwargs
            )
        else:
            outputs = self.qwen_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                **kwargs
            )
        return outputs

    def generate(self, input_ids, attention_mask, images=None, max_new_tokens=128, **kwargs):
        """
        For text generation (multi-modal or text-only).
        """
        print(f"[DEBUG QwenSamBackbone] generate() input_ids shape: {input_ids.shape}")
        print(f"[DEBUG QwenSamBackbone] generate() attention_mask shape: {attention_mask.shape}")
        if images is not None:
            print(f"[DEBUG QwenSamBackbone] generate() images shape: {images.shape}")

            # Calculate the grid_thw parameter for generation based on image dimensions and patch size
            image_grid_thw = self._calculate_grid_thw(images)
        else:
            image_grid_thw = None

        if images is not None:
            outputs = self.qwen_model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=images,
                image_grid_thw=image_grid_thw,  # Pass the calculated grid_thw
                max_new_tokens=max_new_tokens,
                **kwargs
            )
        else:
            outputs = self.qwen_model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                **kwargs
            )
        return outputs

###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/engine/trainer.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/engine/trainer.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.

import os
import time
import math
import csv
import wandb
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
from datetime import datetime
from torch.cuda.amp import autocast, GradScaler

from util.logger import log_validation_samples
from util.utils import set_seed, ensure_dir
from engine.evaluator import Evaluator

class Trainer:
    def __init__(self,
                 model,
                 train_dataloader,
                 val_dataloader=None,
                 lr=1e-4,
                 max_epochs=3,
                 grad_acc_steps=1,
                 scheduler_type="linear",
                 warmup_steps=1000,
                 early_stop_patience=5,
                 clip_grad_norm=1.0,
                 output_dir="runs",
                 run_name="sam4mllm",
                 device="cuda",
                 use_amp=True,
                 log_interval=10):
        self.model = model
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader

        self.lr = lr
        self.max_epochs = max_epochs
        self.grad_acc_steps = grad_acc_steps
        self.scheduler_type = scheduler_type
        self.warmup_steps = warmup_steps
        self.early_stop_patience = early_stop_patience
        self.clip_grad_norm = clip_grad_norm

        self.output_dir = output_dir
        dt_string = datetime.now().strftime("%Y%m%d-%H%M%S")
        self.run_dir = os.path.join(self.output_dir, f"{run_name}_{dt_string}")
        ensure_dir(self.run_dir)

        self.device = device
        self.use_amp = use_amp
        self.log_interval = log_interval

        self.global_step = 0
        self.best_val_loss = float("inf")
        self.epochs_no_improve = 0
        self.should_stop = False

        # Move model to device
        self.model.to(self.device)

        # Setup wandb
        wandb.init(project="SAM4MLLM", name=run_name, dir=self.run_dir)

        # Setup logging files
        self.train_log_path = os.path.join(self.run_dir, "train_log.csv")
        self.val_log_path = os.path.join(self.run_dir, "val_log.csv")

        with open(self.train_log_path, "w", newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["step", "epoch", "loss", "lr"])
        with open(self.val_log_path, "w", newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["step", "epoch", "val_loss"])

        # Setup optimizer
        # Only LoRA parameters are "trainable" in the Qwen model
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        self.optimizer = optim.AdamW(trainable_params, lr=float(self.lr))

        # Setup scheduler
        num_training_steps = len(self.train_dataloader) // self.grad_acc_steps * self.max_epochs
        self.lr_scheduler = self._create_scheduler(self.optimizer, num_training_steps)

        # Setup grad scaler
        self.scaler = GradScaler(enabled=self.use_amp)

        # Evaluator
        if self.val_dataloader is not None:
            self.evaluator = Evaluator(self.model, self.device)
        else:
            self.evaluator = None

        # Folder for saving sample predictions
        self.val_samples_dir = os.path.join(self.run_dir, "val_samples")
        ensure_dir(self.val_samples_dir)

    def _create_scheduler(self, optimizer, num_training_steps):
        if self.scheduler_type == "linear":
            def lr_lambda(current_step: int):
                if current_step < self.warmup_steps:
                    return float(current_step) / float(max(1, self.warmup_steps))
                return max(
                    0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - self.warmup_steps))
                )
            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        elif self.scheduler_type == "cosine":
            def lr_lambda(current_step: int):
                if current_step < self.warmup_steps:
                    return float(current_step) / float(max(1, self.warmup_steps))
                progress = float(current_step - self.warmup_steps) / float(max(1, num_training_steps - self.warmup_steps))
                return 0.5 * (1.0 + math.cos(math.pi * progress))
            scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        else:
            raise ValueError(f"Unknown scheduler: {self.scheduler_type}")
        return scheduler

    def train(self):
        for epoch in range(self.max_epochs):
            print(f"\n=== [Epoch {epoch+1}/{self.max_epochs}] ===")
            self.model.train()
            epoch_loss = 0.0
            step_count = 0

            for step, batch in enumerate(tqdm(self.train_dataloader, desc="Training")):
                # Move to device
                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)
                images = batch.get("image", None)
                if images is not None and isinstance(images, torch.Tensor):
                    images = images.to(self.device)

                loss_val = self._training_step(input_ids, attention_mask, images)
                epoch_loss += loss_val
                step_count += 1

                if (step + 1) % self.log_interval == 0:
                    avg_loss = epoch_loss / step_count
                    lr_now = self.optimizer.param_groups[0]["lr"]
                    print(f"Epoch [{epoch+1}] Step [{step+1}/{len(self.train_dataloader)}], "
                          f"Loss: {avg_loss:.4f}, LR: {lr_now:.6f}")
                    wandb.log({"train/loss": avg_loss, "train/lr": lr_now, "step": self.global_step})
                    with open(self.train_log_path, "a", newline='') as f:
                        writer = csv.writer(f)
                        writer.writerow([self.global_step, epoch+1, avg_loss, lr_now])

            # End of epoch
            if self.val_dataloader is not None:
                val_loss = self.validate(epoch+1)
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.epochs_no_improve = 0
                    self._save_checkpoint(is_best=True, epoch=epoch+1)
                else:
                    self.epochs_no_improve += 1
                    if self.epochs_no_improve >= self.early_stop_patience:
                        print("Early stopping triggered!")
                        self.should_stop = True
                        break
            else:
                self._save_checkpoint(is_best=False, epoch=epoch+1)

            if self.should_stop:
                break

        # Save final
        self._save_checkpoint(is_best=False, epoch=self.max_epochs, last=True)

    def _training_step(self, input_ids, attention_mask, images):
        # We let the huggingface trainer do cross-entropy if we pass `labels`.
        # So we'll pass `labels=input_ids` in a typical "teacher forcing" next token style
        # but it requires we shift the tokens ourselves or rely on the built-in logic.
        with autocast(enabled=self.use_amp):
            # The HF CausalLM automatically uses next-token if we pass labels=input_ids
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                images=images,
                labels=input_ids
            )
            loss = outputs.loss if hasattr(outputs, "loss") else outputs[0]

        loss_for_backward = loss / self.grad_acc_steps
        self.scaler.scale(loss_for_backward).backward()

        if (self.global_step + 1) % self.grad_acc_steps == 0:
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad_norm)
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.optimizer.zero_grad()
            self.lr_scheduler.step()

        self.global_step += 1
        return loss.item()

    def validate(self, epoch):
        self.model.eval()
        val_losses = []
        # We'll store the samples for later logging
        sample_storage = []
        with torch.no_grad():
            for i, batch in enumerate(tqdm(self.val_dataloader, desc=f"Validation Epoch {epoch}")):
                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)
                images = batch.get("image", None)
                if images is not None and isinstance(images, torch.Tensor):
                    images = images.to(self.device)

                with autocast(enabled=self.use_amp):
                    outputs = self.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        images=images,
                        labels=input_ids
                    )
                    loss = outputs.loss if hasattr(outputs, "loss") else outputs[0]
                    val_losses.append(loss.item())

                # Store the first sample in this batch for logging
                if i < 3:  # we log up to 3
                    sample_storage.append({
                        "input_ids": input_ids[0].detach().cpu(),
                        "attention_mask": attention_mask[0].detach().cpu(),
                        "points_str": batch.get("points_str", None)
                    })

        val_loss = sum(val_losses) / len(val_losses) if len(val_losses) > 0 else 0.0
        with open(self.val_log_path, "a", newline='') as f:
            writer = csv.writer(f)
            writer.writerow([self.global_step, epoch, val_loss])

        print(f"[Validation] Epoch: {epoch}, Loss: {val_loss:.4f}")
        wandb.log({"val/loss": val_loss, "epoch": epoch, "step": self.global_step})

        # Now log the 3 samples
        log_validation_samples(
            model=self.model,
            val_samples=sample_storage,
            device=self.device,
            global_step=self.global_step,
            out_dir=self.val_samples_dir,
            tokenizer=getattr(self.model.backbone, "qwen_tokenizer", None)
        )
        return val_loss

    def _save_checkpoint(self, is_best=False, epoch=0, last=False):
        name = "best_model.pt" if is_best else "last_model.pt" if last else f"checkpoint_epoch_{epoch}.pt"
        save_path = os.path.join(self.run_dir, name)
        torch.save({
            "epoch": epoch,
            "global_step": self.global_step,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.lr_scheduler.state_dict(),
            "scaler_state_dict": self.scaler.state_dict()
        }, save_path)
        print(f"Checkpoint saved to {save_path}")


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/engine/evaluator.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/engine/evaluator.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.

import torch
import torch.nn as nn

class Evaluator:
    """
    Minimal evaluator skeleton for SAM4MLLM.
    We do not do advanced metrics here. We let the trainer handle
    the cross-entropy loss. This class can be expanded if needed.
    """
    def __init__(self, model: nn.Module, device="cuda"):
        self.model = model
        self.device = device

    def evaluate(self, dataloader):
        self.model.eval()
        losses = []
        with torch.no_grad():
            for batch in dataloader:
                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)
                images = batch.get("image", None)
                if images is not None and isinstance(images, torch.Tensor):
                    images = images.to(self.device)

                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    images=images,
                    labels=input_ids
                )
                loss = outputs.loss if hasattr(outputs, "loss") else outputs[0]
                losses.append(loss.item())

        mean_loss = sum(losses) / len(losses) if len(losses) > 0 else 0
        return {"loss": mean_loss}


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/util/logger.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/util/logger.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.

import wandb
import torch

def parse_points_from_text(text):
    """
    Attempt to parse lines like '(12,34), (56,78)' from the predicted text
    Return a set of (x, y) tuples.
    """
    # naive parse
    import re
    pattern = r"\(\s*(\d+)\s*,\s*(\d+)\s*\)"
    matches = re.findall(pattern, text)
    points = set()
    for match in matches:
        x = int(match[0])
        y = int(match[1])
        points.add((x, y))
    return points

def compute_point_match(pred_set, gt_str):
    """
    Count how many points in pred_set also appear in the ground truth.
    The ground truth is a string like '(x1,y1), (x2,y2)...' or "NoValidPoints".
    """
    if not gt_str or gt_str == "NoValidPoints":
        return 1.0 if (len(pred_set) == 0) else 0.0

    gt_points = parse_points_from_text(gt_str)
    if len(gt_points) == 0:
        # No GT
        if len(pred_set) == 0:
            return 1.0
        else:
            return 0.0

    # measure fraction of GT that was predicted
    correct = len(pred_set.intersection(gt_points))
    total = len(gt_points)
    return float(correct) / float(total) if total > 0 else 0.0

def log_validation_samples(model, val_samples, device, global_step, out_dir, tokenizer=None):
    """
    For each sample in val_samples, decode the model's predictions and compute
    a naive "point match" with the ground truth string.
    """
    if not val_samples:
        return

    model.eval()
    for idx, sample in enumerate(val_samples):
        input_ids = sample["input_ids"].unsqueeze(0).to(device)
        attention_mask = sample["attention_mask"].unsqueeze(0).to(device)
        gt_str = sample.get("points_str", None)
        if gt_str is not None and isinstance(gt_str, list):
            gt_str = gt_str[0]  # if it was batched

        with torch.no_grad():
            gen_tokens = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=50
            )
        if tokenizer is not None:
            pred_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)
        else:
            # fallback
            pred_text = str(gen_tokens[0].tolist())

        # parse points
        pred_points = parse_points_from_text(pred_text)
        match_score = compute_point_match(pred_points, gt_str)

        # Log to wandb
        log_dict = {
            f"val_sample_{idx}/pred_text": pred_text,
            f"val_sample_{idx}/ground_truth_points": gt_str,
            f"val_sample_{idx}/match_score": match_score,
            "global_step": global_step
        }
        wandb.log(log_dict)


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/util/metrics.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/util/metrics.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.

def compute_iou(pred_mask, gt_mask):
    """
    Example IoU computation placeholder.
    pred_mask, gt_mask: (H, W) boolean or 0/1 arrays
    """
    intersection = (pred_mask & gt_mask).sum()
    union = (pred_mask | gt_mask).sum()
    if union == 0:
        return 1.0
    return float(intersection) / float(union)


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/util/utils.py
# /home/dbcloud/PycharmProjects/mllm4sam/app/util/utils.py
# Copyright (c) 2024, NVIDIA CORPORATION.
# All rights reserved.

import os
import random
import numpy as np
import torch

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def ensure_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)


###################################################################

# /home/dbcloud/PycharmProjects/mllm4sam/app/dataloader/dataset_sam4mllm.py
import os
import csv
import random
import torch
import numpy as np
import PIL
from torch.utils.data import Dataset

def mask_to_points(mask_array, max_points=10):
    """
    Convert a binary segmentation mask to a list of (x,y) pixel coordinates
    lying inside the mask. Return up to max_points of them.
    """
    coords = np.argwhere(mask_array == 1)  # shape (N, 2)
    if len(coords) == 0:
        return []
    coords_list = coords.tolist()
    random.shuffle(coords_list)
    coords_list = coords_list[:max_points]
    return coords_list

class BaseSAM4MLLMDataset(Dataset):
    """
    A base dataset for SAM4MLLM demonstration that also
    extracts random points from the segmentation mask as "ground truth."
    We will present these points in the "assistant" portion to do
    next-token cross-entropy training on Qwen's text output.

    This version addresses:
     - Forcing the image to 224 x 224 to match Qwen2-VL shape expectations.
     - Additional debug prints for shape mismatch issues.
     - Optional CSV loading for 'woundsegmentation' scenario.
     - Using system_prompt to unify conversation text.

    If the user sets `use_data: "woundsegmentation"`, then we attempt to load
    a CSV with image/mask pairs from root_dir. Otherwise, we fallback to a
    user-provided data_list or a dummy example.
    """

    def __init__(self,
                 data_list=None,
                 tokenizer=None,
                 transforms=None,
                 max_len=1536,
                 img_size=(224, 224),   # We now default to 224x224
                 img_dir='./data_images/',
                 system_prompt="You are a helpful segmentation assistant.",
                 use_data="dummy",
                 root_dir="",
                 split="train"):
        super().__init__()
        self.tokenizer = tokenizer
        self.transforms = transforms
        self.max_len = max_len
        # We force 224 x 224 by default for Qwen2-VL
        self.img_size = img_size
        self.img_dir = img_dir
        self.system_prompt = system_prompt
        self.use_data = use_data
        self.root_dir = root_dir
        self.split = split

        # Load or set data_list
        if data_list is not None:
            self.data_list = data_list
        else:
            self.data_list = []
            if self.use_data == "woundsegmentation":
                self.data_list = self._load_woundsegmentation_data()
            else:
                # fallback dummy
                self.data_list = [
                    {"image_path": "dummy1.png", "mask_path": "dummy1_mask.png", "conversation": "Segment?"},
                    {"image_path": "dummy2.png", "mask_path": "dummy2_mask.png", "conversation": "Segment?"}
                ]

        self._prune_missing_files()

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx: int):
        item = self.data_list[idx]
        image_path = item.get("image_path", None)
        mask_path = item.get("mask_path", None)
        conversation = item.get("conversation", "")

        if not image_path:
            raise ValueError(f"No 'image_path' found for index {idx} in data_list.")

        # 1. Load image
        full_image_path = self._resolve_path(image_path, subfolder="images")
        if not os.path.exists(full_image_path):
            raise FileNotFoundError(f"Image file not found even after pruning: {full_image_path}")
        print(f"[DEBUG Dataset] Loading image: {full_image_path}")

        image = PIL.Image.open(full_image_path).convert("RGB")

        # 2. Load mask if mask_path is available
        mask = None
        if mask_path:
            full_mask_path = self._resolve_path(mask_path, subfolder="masks")
            if os.path.exists(full_mask_path):
                print(f"[DEBUG Dataset] Loading mask: {full_mask_path}")
                mask = PIL.Image.open(full_mask_path).convert("L")
            else:
                print(f"[DEBUG Dataset] No mask found at {full_mask_path}, skipping mask.")
                mask = None

        # 3. transforms or resizing to 224x224
        #    Qwen2-VLs patch-embedding requires (B, 3, 224, 224) by default
        if self.transforms:
            image = self.transforms(image)
            if mask is not None:
                mask = self.transforms(mask)
        else:
            image = image.resize(self.img_size)
            if mask is not None:
                mask = mask.resize(self.img_size)

        # 4. Convert mask to up to 10 points
        max_points = 10
        points_str = ""
        if mask is not None:
            mask_np = np.array(mask, dtype=np.uint8)
            mask_np = (mask_np >= 128).astype(np.uint8)
            coords_list = mask_to_points(mask_np, max_points=max_points)
            if len(coords_list) > 0:
                points_str_list = []
                for (y, x) in coords_list:
                    points_str_list.append(f"({x},{y})")
                points_str = ", ".join(points_str_list)
            else:
                points_str = "NoValidPoints"
        else:
            # If no mask, we have no valid points
            points_str = "NoValidPoints"

        # 5. Build textual prompt
        user_text = f"Please provide up to 10 points that cover the object region."
        assistant_text = points_str
        full_text = (
            f"{self.system_prompt}\n[USER]: {conversation}\n"
            f"[USER]: {user_text}\n"
            f"[ASSISTANT]: {assistant_text}"
        )

        # 6. Tokenize if available
        if self.tokenizer is not None:
            tokens = self.tokenizer(
                full_text,
                return_tensors="pt",
                truncation=True,
                max_length=self.max_len
            )
            input_ids = tokens["input_ids"].squeeze(0)
            attention_mask = tokens["attention_mask"].squeeze(0)
        else:
            # placeholders
            input_ids = torch.tensor([0])
            attention_mask = torch.tensor([1])

        # 7. Convert image to tensor if still PIL
        if isinstance(image, PIL.Image.Image):
            image = torch.from_numpy(np.array(image)).permute(2, 0, 1)
        # Debug shape
        print(f"[DEBUG Dataset] Final image shape: {image.shape}")

        sample = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "image": image,
            "points_str": points_str
        }
        return sample

    def _resolve_path(self, filename, subfolder="images"):
        """
        1) Fix common issues if 'train/images/' or 'test/images/' are already in 'filename'
           so we don't double them.
        2) Attempt to see if it's an absolute path. If so, return it.
        3) Otherwise, check if it's in self.img_dir
        4) Else, fallback to root_dir/split/subfolder/filename
        """
        fix_list = [
            "train/images/", "test/images/",
            "train/masks/", "test/masks/"
        ]
        for fix_token in fix_list:
            if fix_token in filename:
                # print(f"[DEBUG Dataset] Stripping '{fix_token}' from filename: {filename}")
                filename = filename.replace(fix_token, "")

        # Now proceed
        if os.path.isabs(filename):
            return filename

        potential_path = os.path.join(self.img_dir, filename)
        if os.path.exists(potential_path):
            return potential_path

        alt = os.path.join(self.root_dir, self.split, subfolder, filename)
        return alt

    def _load_woundsegmentation_data(self):
        """
        If `use_data == "woundsegmentation"`, we read from:
          root_dir/train_labels.csv or root_dir/test_labels.csv
        to build data_list with image_path, mask_path, conversation, etc.
        """
        data_list = []
        csv_name = "train_labels.csv" if self.split == "train" else "test_labels.csv"
        csv_path = os.path.join(self.root_dir, csv_name)
        if not os.path.exists(csv_path):
            print(f"[WARNING] CSV file not found at {csv_path}. Returning empty data_list.")
            return data_list

        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            for row in reader:
                if not row:
                    continue
                filename = row[0].strip()
                data_item = {
                    "image_path": filename,
                    "mask_path": filename,
                    "conversation": "Help me segment this wound."
                }
                data_list.append(data_item)
        return data_list

    def _prune_missing_files(self):
        kept = []
        for item in self.data_list:
            resolved = self._resolve_path(item["image_path"], subfolder="images")
            if not os.path.exists(resolved):
                print(f"[WARNING] Skipping nonexistent file: {resolved}")
                continue
            kept.append(item)
        self.data_list = kept
        print(f"[INFO] After pruning, we have {len(self.data_list)} valid samples.")


###################################################################

<my code>